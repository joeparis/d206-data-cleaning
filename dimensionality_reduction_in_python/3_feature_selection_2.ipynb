{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection II - Selecting for Model Accuracy\n",
    "\n",
    "In this second chapter on feature selection, you'll learn how to let models help you find the most important features in a dataset for predicting a particular target feature. In the final lesson of this chapter, you'll combine the advice of multiple, different, models to decide on which features are worth keeping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting features for model performance\n",
    "\n",
    "So far we've only considered individual or pairwise properties of features to decide whether we keep them in the dataset or not.\n",
    "\n",
    "Another approach is to select features based on how they affect model performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ansur dataset sample\n",
    "\n",
    "Consider this sample of the ANSUR dataset with one target variable, \"Gender\" which we'll try to predict, and five body measurement features to do so. \n",
    "\n",
    "| Gender | chestdepth | handlength | neckcircumference | shoulderlength | earlength |\n",
    "|--------|-----------:|-----------:|------------------:|---------------:|----------:|\n",
    "| Female | 243 | 176 | 326 | 136 | 62 |\n",
    "| Female | 219 | 177 | 325 | 135 | 58 |\n",
    "| Male | 259 | 193 | 400 | 145 | 71 |\n",
    "| Male | 253 | 195 | 380 | 141 | 62 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing the data\n",
    "\n",
    "To train a model on this data we should first perform a train-test-split, and in this case also standardize the training feature dataset `X_train` to have a mean of zero and a variance of one. Notice that we've used the `.fit_transform()` method to fit the scaler and transform the data in one command.\n",
    "\n",
    "``` python\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_train, X_test, y_train, t_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_std = scaler.fit_tramsform(X_train)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a logistic regression model\n",
    "\n",
    "We can then create and fit a logistic regression model on this standardized training data. To see how the model performs on the test set we first scale these features with the `.transform()` method of the scaler that we just fit on the training set and then make our prediction. we get a test-set accuracy of 99%. \n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train_std, y_train)\n",
    "\n",
    "X_test_std = scaler.transform(X_test)\n",
    "\n",
    "y_pred = lr.predict(X_test_std)\n",
    "\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "```\n",
    "\n",
    "```sh\n",
    "0.99\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting the feature coefficients\n",
    "\n",
    "However, when we look at the feature coefficients that the logistic regression model uses in its decision function, we'll see that some values are pretty close to zero.\n",
    "\n",
    "```python\n",
    "print(lr.coef_)\n",
    "```\n",
    "\n",
    "```sh\n",
    "array([[-3. , 0.14, 7.46, 1.22, 0.87]])\n",
    "```\n",
    "\n",
    "Since these coefficients will be multiplied with the feature values when the model makes a prediction, features with coefficients close to zero will contribute little to the end result.\n",
    "\n",
    "We can use the zip function to transform the output into a dictionary that shows which feature has which coefficient.\n",
    "\n",
    "```python\n",
    "print(dict(zip(X.columns, abs(lr.coef_[0]))))\n",
    "```\n",
    "\n",
    "```sh\n",
    "{'chestdepth': 3.0,\n",
    " 'handlength': 0.14,\n",
    " 'neckcircumference': 7.46,\n",
    " 'shoulderlength': 1.22,\n",
    " 'earlength': 0.87}\n",
    "```\n",
    "\n",
    "If we want to remove a feature from the initial dataset with as little effect on the predictions as possible, we should pick the one with the lowest coefficient, \"handlength\" in this case. **The fact that we standardized the data first makes sure that we can compare the coefficients to one another.**\n",
    "\n",
    "### Features that contribute little to a model\n",
    "\n",
    "When we remove the \"handlength\" feature at the start of the process, our model accuracy remains unchanged at 99% while we did reduce our dataset's complexity.\n",
    "\n",
    "```python\n",
    "X.drop('handlength', axes=1, inplace=True)\n",
    "\n",
    "X_train, X_test, y_train, t_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "lr.fit(scaler.fit_transform(X_train), y_train)\n",
    "\n",
    "print(accuracy_score(y_test, lr.predict(scaler.transform(X_test))))\n",
    "```\n",
    "\n",
    "```sh\n",
    "0.99\n",
    "```\n",
    "\n",
    "We could repeat this process until we have the desired number of features remaining, but it turns out, there's a Scikit-learn function that does just that. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recursive Feature Elimination\n",
    "\n",
    "Recursive Feature Elimination (RFE) is a feature selection algorithm that can be wrapped around any model that produces feature coefficients or feature importance values. We can pass it the model we want to use and the number of features we want to select. While fitting to our data it will repeat a process where it first fits the internal model and then drops the feature with the weakest coefficient. It will keep doing this until the desired number of features is reached. If we set RFE's verbose parameter higher than zero we'll be able to see that features are dropped one by one while fitting. We could also decide to just keep the 2 features with the highest coefficients after fitting the model once, but this recursive method is safer, since **dropping one feature will cause the other coefficients to change**. \n",
    "\n",
    "```python\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "rfe = RFE(estimator=LogisticRegression(), n_features_to_select=2, verbose=1)\n",
    "rfe.fit(X_train_std, y_train)\n",
    "```\n",
    "\n",
    "```sh\n",
    "Fitting estimator with 5 features.\n",
    "Fitting estimator with 4 features.\n",
    "Fitting estimator with 3 features.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting the RFE results\n",
    "\n",
    "Once RFE is done we can check the `support_` attribute that contains `True`/`False` values to see which features were kept in the dataset.\n",
    "\n",
    "```python\n",
    "X.columns[rfe.support_]\n",
    "```\n",
    "\n",
    "```sh\n",
    "Index(['chestdepth', 'neckcircumference'], dtype='object')\n",
    "```\n",
    "\n",
    "Using the `zip` function once more, we can also check out rfe's `ranking_` attribute to see in which iteration a feature was dropped. Values of 1 mean that the feature was kept in the dataset until the end while high values mean the feature was dropped early on.\n",
    "\n",
    "```python\n",
    "print(dict(zip(X.columns, rfe.ranking_)))\n",
    "```\n",
    "\n",
    "```sh\n",
    "{'chestdepth': 1,\n",
    " 'handlength': 4,\n",
    " 'neckcircumference': 1,\n",
    " 'shoulderlength': 2,\n",
    " 'earlength': 3}\n",
    "```\n",
    "\n",
    "Finally, we can test the accuracy of the model with just two remaining features, `'chestdepth'` and `'neckcircumference'`, turns out the accuracy is still untouched at 99%. This means the other 3 features had little to no impact on the model an its predictions. \n",
    "\n",
    "```python\n",
    "print(accuracy_score(y_test, rfe.predict(X_test_std)))\n",
    "```\n",
    "\n",
    "```sh\n",
    "0.99\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building a diabetes classifier\n",
    "\n",
    "You'll be using the Pima Indians diabetes dataset to predict whether a person has diabetes using logistic regression. There are 8 features and one target in this dataset. The data has been split into a training and test set and pre-loaded for you as `X_train`, `y_train`, `X_test`, and `y_test`.\n",
    "\n",
    "A `StandardScaler()` instance has been predefined as `scaler` and a `LogisticRegression()` one as `lr`.\n",
    "\n",
    "##### Instructions\n",
    "\n",
    "* Fit the scaler on the training features and transform these features in one go.\n",
    "* Fit the logistic regression model on the scaled training data.\n",
    "* Scale the test features.\n",
    "* Predict diabetes presence on the scaled test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "diabetes_df = pd.read_csv(\"data/pima_indians_diabetes_2.csv\")\n",
    "\n",
    "y = diabetes_df[\"test\"]\n",
    "X = diabetes_df.drop(\"test\", axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "lr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the scaler on the training features and transform in one go\n",
    "X_train_std = scaler.fit_transform(X_train)\n",
    "\n",
    "# fit the logistic regression model on the scaled training data\n",
    "lr.fit(X_train_std, y_train)\n",
    "\n",
    "# scale the test features\n",
    "X_test_std = scaler.transform(X_test)\n",
    "\n",
    "# predict diabetes presence on the scaled test set\n",
    "y_pred = lr.predict(X_test_std)\n",
    "\n",
    "# Prints accuracy metrics and feature coefficients\n",
    "print(f\"{accuracy_score(y_test, y_pred):.1%} accuracy on test set.\")\n",
    "print(dict(zip(X.columns, abs(lr.coef_[0]).round(2))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manual recursive feature elimination\n",
    "\n",
    "Now that we've created a diabetes classifier, let's see if we can reduce the number of features without hurting the model accuracy too much.\n",
    "\n",
    "On the second line of code the features are selected from the original DataFrame. Adjust this selection.\n",
    "\n",
    "A `StandardScaler()` instance has been predefined as `scaler` and a `LogisticRegression()` one as `lr`.\n",
    "\n",
    "All necessary functions and packages have been pre-loaded too.\n",
    "\n",
    "##### Instructions 1/3\n",
    "\n",
    "* First, run the given code, then remove the feature with the lowest model coefficient from X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the feature with the lowest model coefficient\n",
    "# X = diabetes_df[[\"pregnant\", \"glucose\", \"diastolic\", \"triceps\", \"insulin\", \"bmi\", \"family\", \"age\"]]\n",
    "X = diabetes_df[[\"pregnant\", \"glucose\", \"triceps\", \"insulin\", \"bmi\", \"family\", \"age\"]]\n",
    "\n",
    "# performs a 25-75% train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n",
    "\n",
    "# scales features and fits the logistic regression model\n",
    "lr.fit(scaler.fit_transform(X_train), y_train)\n",
    "\n",
    "# calculates the accuracy on the test set and prints coefficients\n",
    "acc = accuracy_score(y_test, lr.predict(scaler.transform(X_test)))\n",
    "print(f\"{acc:.1%} accuracy on test set.\")\n",
    "print(dict(zip(X.columns, abs(lr.coef_[0]).round(2))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Instructions 2/3\n",
    "\n",
    "* Run the code and remove 2 more features with the lowest model coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the 2 features with the lowest model coefficients\n",
    "# X = diabetes_df[[\"pregnant\", \"glucose\", \"triceps\", \"insulin\", \"bmi\", \"family\", \"age\"]]\n",
    "X = diabetes_df[[\"glucose\", \"triceps\", \"bmi\", \"family\", \"age\"]]\n",
    "\n",
    "# Performs a 25-75% train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n",
    "\n",
    "# Scales features and fits the logistic regression model\n",
    "lr.fit(scaler.fit_transform(X_train), y_train)\n",
    "\n",
    "# Calculates the accuracy on the test set and prints coefficients\n",
    "acc = accuracy_score(y_test, lr.predict(scaler.transform(X_test)))\n",
    "print(f\"{acc:.1%} accuracy on test set.\")\n",
    "print(dict(zip(X.columns, abs(lr.coef_[0]).round(2))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Instructions 3/3\n",
    "\n",
    "* Run the code and only keep the feature with the highest coefficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only keep the feature with the highest coefficient\n",
    "# X = diabetes_df[[\"glucose\", \"triceps\", \"bmi\", \"family\", \"age\"]]\n",
    "X = diabetes_df[[\"glucose\"]]\n",
    "\n",
    "# Performs a 25-75% train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n",
    "\n",
    "# Scales features and fits the logistic regression model to the data\n",
    "lr.fit(scaler.fit_transform(X_train), y_train)\n",
    "\n",
    "# Calculates the accuracy on the test set and prints coefficients\n",
    "acc = accuracy_score(y_test, lr.predict(scaler.transform(X_test)))\n",
    "print(f\"{acc:.1%} accuracy on test set.\")\n",
    "print(dict(zip(X.columns, abs(lr.coef_[0]).round(2))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Automatic recursive feature elimination\n",
    "\n",
    "Now let's automate this recursive process. Wrap a Recursive Feature Eliminator (RFE) around our logistic regression estimator and pass it the desired number of features.\n",
    "\n",
    "All the necessary functions and packages have been pre-loaded and the features have been scaled for you.\n",
    "\n",
    "##### Instructions\n",
    "\n",
    "* Create the RFE with a `LogisticRegression()` estimator and 3 features to select.\n",
    "* Print the features and their ranking.\n",
    "* Print the features that are not eliminated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "diabetes_df = pd.read_csv(\"data/pima_indians_diabetes_2.csv\")\n",
    "# diabetes_df = diabetes_df.drop([\"pregnant\"], axis=1)\n",
    "\n",
    "y = diabetes_df[\"test\"]\n",
    "X = diabetes_df.drop(\"test\", axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "lr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfe = RFE(estimator=LogisticRegression(max_iter=500, random_state=0), n_features_to_select=3, verbose=1)\n",
    "\n",
    "rfe.fit(X_train, y_train)\n",
    "\n",
    "print(dict(zip(X.columns, rfe.ranking_)))\n",
    "\n",
    "print(X.columns[rfe.support_])\n",
    "\n",
    "acc = accuracy_score(y_test, rfe.predict(X_test))\n",
    "print(f\"{acc:.1%} accuracy on test set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The site and I are getting different results. They consistently get:\n",
    "\n",
    "```sh\n",
    "{'pregnant': 5, 'glucose': 1, 'diastolic': 6, 'triceps': 3, 'insulin': 4, 'bmi': 1, 'family': 2, 'age': 1} \n",
    "Index(['glucose', 'bmi', 'age'], dtype='object')\n",
    "80.6% accuracy on test set.\n",
    "```\n",
    "\n",
    "While my results vary but almost always include `'pregnant'` unless I drop it from the dataset.\n",
    "\n",
    "According to my experiments below, the site and I are producing identical correlation matrices and our heatmaps are not surprisingly the same as well.\n",
    "\n",
    "Interestingly, if I don't increase the `max_iter` parameter I get the following *after* my results:\n",
    "\n",
    "```sh\n",
    "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
    "\n",
    "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
    "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
    "Please also refer to the documentation for alternative solver options:\n",
    "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
    "  n_iter_i = _check_optimize_result(\n",
    "```\n",
    "\n",
    "The value I have to set for `max_iter` is not constant but I've never seen the error with a value >= 200. I am now wondering if the default solver has changed?\n",
    "\n",
    "Checking the versions, on the site:\n",
    "\n",
    "```sh\n",
    "In [17]: import sklearn\n",
    "In [18]: print('sklearn: {}'. format(sklearn. __version__))\n",
    "sklearn: 1.0\n",
    "```\n",
    "\n",
    "and on my machine:\n",
    "\n",
    "```sh\n",
    ">>> import sklearn\n",
    ">>> print('sklearn: {}'. format(sklearn. __version__))\n",
    "sklearn: 1.3.2\n",
    "```\n",
    "\n",
    "After checking, however, (`print(LogisticRegression().solver)`) both the site and I are using `lbfgs`.\n",
    "\n",
    "Next, I thought I would try installing scikit-learn v1.0 on my machine to see if I can reproduce the site's results. This, however, turned out to be more involved than I expected. Instead, I built a separate env with numpy v1.19.5, scikit-learn v1.0, and Python v3.9.7 to mirror the site's environment.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(LogisticRegression().solver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_df.corr(numeric_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "cmap = sns.diverging_palette(h_neg=10, h_pos=240, as_cmap=True)\n",
    "\n",
    "corr = diabetes_df.corr(numeric_only=True)\n",
    "\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "\n",
    "sns.heatmap(corr, mask=mask, center=0, cmap=cmap, linewidths=1, annot=True, fmt=\".2f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Had to set `max_iter` to prevent an error. \n",
    "\n",
    "My results (`Index(['glucose', 'bmi', 'family'], dtype='object')`) are not matching the site's (`Index(['glucose', 'bmi', 'age'], dtype='object')`). \n",
    "\n",
    "I dropped the `'pregnant'` column because it was selected every time.\n",
    "\n",
    "I think I must be doing something wrong, `'glucose'` is often not selected and `'age'` never is and if `'pregnant'` is not removed it is *always* selected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree-based feature selection\n",
    "\n",
    "Some models will perform feature selection by design to avoid overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest classifier\n",
    "\n",
    "One of those, is the random forest classifier - an ensemble model that will pass different, random, subsets of features to a number of decision trees. It then aggregates the predictions of the individual trees. The example forest shown here contains four decision trees.\n",
    "\n",
    "![alt text](images/forest_of_decision_trees.png)\n",
    "\n",
    "While simple in design, random forests often manage to be highly accurate and avoid overfitting even with the default Scikit-learn settings. \n",
    "\n",
    "Training a random forest classifier on the numeric features of the ANSUR dataset to predict gender, its test set accuracy is 99%. This means it managed to escape the curse of dimensionality and didn't overfit on the many features in the training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "ansur_df = pd.read_csv(\"data/ansur_gender_numeric.csv\")\n",
    "\n",
    "y = ansur_df[\"Gender\"]\n",
    "X = ansur_df.drop(\"Gender\", axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "rf = RandomForestClassifier(random_state=0)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "print(f\"{accuracy_score(y_test, rf.predict(X_test)):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this illustration of what the trained model could look like, the first decision tree in the forest used the neck circumference feature on its first decision node and hand length later on to determine if a person was male or female.\n",
    "\n",
    "![alt text](images/ansur_gender_forest_of_decision_trees.png)\n",
    "\n",
    "By averaging how often features are used to make decisions inside the different decision trees, and taking into account whether these are important decisions near the root of the tree or less important decisions in the smaller branches of the tree, the random forest algorithm manages to calculate feature importance values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature importance values\n",
    "\n",
    "These values can be extracted from a trained model using the `feature_importances_` attribute. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rf.feature_importances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like the coefficients produced by the logistic regressor, these feature importance values can be used to perform feature selection, since for unimportant features they will be close to zero. An advantage of these feature importance values over coefficients is that they are comparable between features by default, since they always sum up to one. Which means we don't have to scale our input data first. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature importance as a feature selector\n",
    "\n",
    "We can use the feature importance values to create a `True`/`False` mask for features that meet a certain importance threshold. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = rf.feature_importances_ > 0.1\n",
    "\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can apply that mask to our feature DataFrame to implement the actual feature selection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_reduced = X.loc[:, mask]\n",
    "\n",
    "print(X_reduced.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RFE with random forests\n",
    "\n",
    "Dropping one weak feature can make other features relatively more or less important.\n",
    "\n",
    "To play it safe and minimize the risk of dropping the wrong features, you should not drop all least important features at once but rather one by one. To do so we can once again wrap a **Recursive Feature Eliminator**, or `RFE()`, around our model.\n",
    "\n",
    "Here, we've reduced the number of features to six with no reduction in test set accuracy. However, training the model once for each feature we want to drop can result in too much computational overhead. \n",
    "\n",
    "To speed up the process we can pass the `step` parameter to `RFE()`. Here, we've set it to 10 so that on each iteration the 10 least important features are dropped. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "y = ansur_df[\"Gender\"]\n",
    "X = ansur_df.drop(\"Gender\", axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "# rfe = RFE(estimator=RandomForestClassifier(random_state=0), n_features_to_select=6, verbose=1)\n",
    "rfe = RFE(estimator=RandomForestClassifier(random_state=0), n_features_to_select=6, step=10, verbose=1)\n",
    "\n",
    "rfe.fit(X_train, y_train)\n",
    "\n",
    "# print(accuracy_score(y_test, rfe.predict(X_test)))\n",
    "print(f\"{accuracy_score(y_test, rfe.predict(X_test)):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the final model is trained, we can use the feature eliminator's .support_ attribute as a mask to print the remaining column names. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.columns[rfe.support_])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building a random forest model\n",
    "\n",
    "You'll again work on the Pima Indians dataset to predict whether an individual has diabetes. This time using a random forest classifier. You'll fit the model on the training data after performing the train-test split and consult the feature importance values.\n",
    "\n",
    "The feature and target datasets have been pre-loaded for you as `X` and `y`. Same goes for the necessary packages and functions.\n",
    "\n",
    "##### Instructions\n",
    "\n",
    "* Set a 25% test size to perform a 75%-25% train-test split.\n",
    "* Fit the random forest classifier to the training data.\n",
    "* Calculate the accuracy on the test set.\n",
    "* Print the feature importances per feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "ansur_df = pd.read_csv(\"data/pima_indians_diabetes_2.csv\")\n",
    "\n",
    "y = ansur_df[\"test\"]\n",
    "X = ansur_df.drop(\"test\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n",
    "\n",
    "rf = RandomForestClassifier(random_state=0)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "acc = accuracy_score(y_test, rf.predict(X_test))\n",
    "\n",
    "print(dict(zip(X.columns, rf.feature_importances_.round(2))))\n",
    "\n",
    "print(f\"{acc:.1%} accuracy on test set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### random forest for feature selection\n",
    "\n",
    "Now lets use the fitted random model to select the most important features from our input dataset `X`.\n",
    "\n",
    "The trained model from the previous exercise has been pre-loaded for you as `rf`.\n",
    "\n",
    "##### Instructions 1/2\n",
    "\n",
    "* Create a mask for features with an importance higher than 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = rf.feature_importances_ > 0.15\n",
    "\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Instructions 2/2\n",
    "\n",
    "* Sub-select the most important features by applying the mask to `X`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_X = X.loc[:, mask]\n",
    "\n",
    "print(reduced_X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recursive feature elimination with random forests \n",
    "\n",
    "You'll wrap a Recursive Feature Eliminator around a random forest model to remove features step by step. This method is more conservative compared to selecting features after applying a single importance threshold. Since dropping one feature can influence the relative importances of the others.\n",
    "\n",
    "You'll need these pre-loaded datasets: `X`, `X_train`, `y_train`.\n",
    "\n",
    "Functions and classes that have been pre-loaded for you are: `RandomForestClassifier()`, `RFE()`, `train_test_split()`.\n",
    "\n",
    "##### Instructions 1/4\n",
    "\n",
    "* Create a recursive feature eliminator that will select the 2 most important features using a random forest model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "ansur_df = pd.read_csv(\"data/pima_indians_diabetes_2.csv\")\n",
    "\n",
    "y = ansur_df[\"test\"]\n",
    "X = ansur_df.drop(\"test\", axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfe = RFE(estimator=RandomForestClassifier(random_state=0), n_features_to_select=2, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Instructions 2/4\n",
    "\n",
    "* Fit the recursive feature eliminator to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Instructions 3/4\n",
    "\n",
    "* Create a mask using the fitted eliminator's `support_` attribute, then apply it to the feature dataset `X`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = rfe.support_\n",
    "\n",
    "reduced_X = X.loc[:, mask]\n",
    "print(reduced_X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Instructions 4/4\n",
    "\n",
    "* Change the settings of `RFE()` to eliminate 2 features at each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfe = RFE(estimator=RandomForestClassifier(), n_features_to_select=2, step=2, verbose=1)\n",
    "\n",
    "rfe.fit(X_train, y_train)\n",
    "\n",
    "mask = rfe.support_\n",
    "\n",
    "reduced_X = X.loc[:, mask]\n",
    "print(reduced_X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularized linear regression\n",
    "\n",
    "So far, we focused on how to reduce dimensionality using classification algorithms. Let's see what we can do with regressions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear model concept\n",
    "\n",
    "To refresh how linear regressions work, we'll build a model that derives the linear function between three input values and a target. \n",
    "\n",
    "However, we'll be creating the feature dataset and linear function ourselves, so that we can control the ground truth that our model tries to derive. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating our own dataset\n",
    "\n",
    "Create three features x1, x2, and x3 that all follow a simple normal distribution.\n",
    "\n",
    "![alt text](images/dataset_distribution.png)\n",
    "\n",
    "We can then create a target y with a function of our choice.\n",
    "\n",
    "$$\n",
    "y = 20 + 5x_1 + 2x_2 + 0x_3 + error\n",
    "$$\n",
    "\n",
    "The $20$ is called the intercept.\n",
    "\n",
    "The $5$, $2$, and $0$ are the coefficients of our features and determine how large an effect each has on the target.\n",
    "\n",
    "Since the third feature has a coefficient of 0 and will have no effect on the target it would be best to remove it so it does not confuse the model and cause overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression in Python\n",
    "\n",
    "When you fit a `LinearRegression()` model with Scikit Learn, the model object will have `.coef_` for coefficient attribute that contains a NumPy array with a number of elements equal to the number of input features. These are the three values we just set to 5, 2, and 0 and the model was able to estimate them pretty accurately.\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "print(lr.coef_)\n",
    "```\n",
    "\n",
    "```sh\n",
    "[ 4.95 1.83 -0.05]\n",
    "```\n",
    "\n",
    "Same goes for the intercept. \n",
    "\n",
    "```python\n",
    "print(lr.intercept_)\n",
    "```\n",
    "\n",
    "```sh\n",
    "19.8\n",
    "```\n",
    "\n",
    "To check how accurate the model's predictions are we can calculate the R-squared value on the test set. This tells us how much of the variance in the target feature our model can predict. \n",
    "\n",
    "```python\n",
    "print(lr.score(X_test, y_test))\n",
    "```\n",
    "\n",
    "```sh\n",
    "0.976\n",
    "```\n",
    "\n",
    "Our model scores an impressive 97.6%. \n",
    "\n",
    "However, the third feature, which had no effect whatsoever, was estimated to have a small effect of -0.05. If there would be more of these irrelevant features, the model could overfit. To solve this, we'll have to look at what the model actually does while training. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function: Mean Squared Error\n",
    "\n",
    "The model will try to find optimal values for the intercept and coefficients by minimizing a loss function. \n",
    "\n",
    "This function contains the mean sum of the squared differences between actual and predicted values, the gray squares in the plot.\n",
    "\n",
    "![alt text](images/mse.png)\n",
    "\n",
    "Minimizing this MSE makes the model as accurate a possible. However, we don't want our model to be super accurate on the training set if that means it no longer generalizes to new data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding regularization\n",
    "\n",
    "To avoid this we can introduce regularization. The model will then not only try to be as accurate as possible by minimizing the MSE, it will also try to keep the model simple by keeping the coefficients low. The strength of regularization can be tweaked with alpha, when it's too low the model might overfit, when it's too high the model might become too simple and inaccurate.\n",
    "\n",
    "$$\\alpha(\\lvert \\Beta_1 \\rvert + \\lvert \\Beta_2 \\rvert + \\lvert \\Beta_3 \\rvert)$$\n",
    "\n",
    "![alt text](images/regularization.png)\n",
    "\n",
    "One linear model that includes this type of regularization is called Lasso, for least absolute shrinkage and selection operator. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso regressor\n",
    "\n",
    "When we fit it on our dataset we see that it indeed reduced the coefficient of the third feature to zero, ignoring it, but also that it reduced the other coefficients resulting in a lower R squared. \n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "la = Lasso()\n",
    "la.fit(X_train, y_train)\n",
    "\n",
    "print(la.coef_)\n",
    "```\n",
    "\n",
    "```sh\n",
    "[4.07 0.59 0. ]\n",
    "```\n",
    "\n",
    "```python\n",
    "print(la.score(X_test, y_test))\n",
    "```\n",
    "\n",
    "```sh\n",
    "0.861\n",
    "```\n",
    "\n",
    "To avoid this we can change the alpha parameter. When we set it to 0.05 the third feature is still ignored but the other coefficients are reduced less and our R squared is up again. \n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import lasso\n",
    "\n",
    "la = lasso(alpha=0.05)\n",
    "la.fit(X_train, y_train)\n",
    "\n",
    "print(la.coef_)\n",
    "```\n",
    "\n",
    "```sh\n",
    "[4.91 1.76 0. ]\n",
    "```\n",
    "\n",
    "```python\n",
    "print(la.score(X_test, y_test))\n",
    "```\n",
    "\n",
    "```sh\n",
    "0.974\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a LASSO regressor\n",
    "\n",
    "You'll be working on the numeric ANSUR body measurements dataset to predict a persons Body Mass Index (BMI) using the pre-imported `Lasso()` regressor. BMI is a metric derived from body height and weight but those two features have been removed from the dataset to give the model a challenge.\n",
    "\n",
    "You'll standardize the data first using the `StandardScaler()` that has been instantiated for you as `scaler` to make sure all coefficients face a comparable regularizing force trying to bring them down.\n",
    "\n",
    "All necessary functions and classes plus the input datasets `X` and `y` have been pre-loaded.\n",
    "\n",
    "##### Instructions\n",
    "\n",
    "* Set the test size to 30% to get a 70-30% train test split.\n",
    "* Fit the scaler on the training features and transform these in one go.\n",
    "* Create the Lasso model.\n",
    "* Fit it to the scaled training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "ansur_df = pd.read_csv(\"data/ansur_numeric.csv\")\n",
    "\n",
    "y = ansur_df[\"BMI\"]\n",
    "X = ansur_df.drop(\"BMI\", axis=1)\n",
    "\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "X_train_std = scaler.fit_transform(X_train, y_train)\n",
    "\n",
    "la = Lasso()\n",
    "\n",
    "la.fit(X_train_std, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lasso model results\n",
    "\n",
    "Now that you've trained the Lasso model, you'll score its predictive capacity ($R^2$) on the test set and count how many features are ignored because their coefficient is reduced to zero.\n",
    "\n",
    "The `X_test` and `y_test` datasets have been pre-loaded for you.\n",
    "\n",
    "The `Lasso()` model and `StandardScaler()` have been instantiated as `la` and `scaler` respectively and both were fitted to the training data.\n",
    "\n",
    "##### Instructions\n",
    "\n",
    "* Transform the test set with the pre-fitted scaler.\n",
    "* Calculate the $R^2$ value on the scaled test data.\n",
    "* Create a list that has True values when coefficients equal 0.\n",
    "* Calculate the total number of features with a coefficient of 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "ansur_df = pd.read_csv(\"data/ansur_numeric.csv\")\n",
    "\n",
    "y = ansur_df[\"BMI\"]\n",
    "X = ansur_df.drop(\"BMI\", axis=1)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "X_train_std = scaler.fit_transform(X_train)\n",
    "\n",
    "la = Lasso()\n",
    "\n",
    "la.fit(X_train_std, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the test set with the pre-fitted scaler\n",
    "X_test_std = scaler.transform(X_test)\n",
    "\n",
    "# calculate the coefficient of determination (R squared) on X_test_std\n",
    "r_squared = la.score(X_test_std, y_test)\n",
    "print(f\"The model can predict {r_squared:.1%} of the variance in the test set.\")\n",
    "\n",
    "# Create a list that has True values when coefficients equal 0\n",
    "zero_coef = la.coef_ == 0\n",
    "\n",
    "# Calculate how many features have a zero coefficient\n",
    "n_ignored = sum(zero_coef)\n",
    "print(f\"The model has ignored {n_ignored} out of {len(la.coef_)} features.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adjusting the regularization strength\n",
    "\n",
    "Your current Lasso model has an $R^2$ score of 84.7%. When a model applies overly powerful regularization it can suffer from high bias, hurting its predictive power.\n",
    "\n",
    "Let's improve the balance between predictive power and model simplicity by tweaking the `alpha` parameter.\n",
    "\n",
    "##### Instructions\n",
    "\n",
    "* Find the highest value for `alpha` that gives an $R^2$ value above 98% from the options: `1`, `0.5`, `0.1`, and `0.01`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the highest alpha value with R-squared above 98%\n",
    "# la = Lasso(alpha=1, random_state=0)\n",
    "# la = Lasso(alpha=0.5, random_state=0)\n",
    "la = Lasso(alpha=0.1, random_state=0)\n",
    "# la = Lasso(alpha=0.01, random_state=0)\n",
    "\n",
    "# Fits the model and calculates performance stats\n",
    "la.fit(X_train_std, y_train)\n",
    "r_squared = la.score(X_test_std, y_test)\n",
    "n_ignored_features = sum(la.coef_ == 0)\n",
    "\n",
    "# Print peformance stats\n",
    "print(f\"The model can predict {r_squared:.1%} of the variance in the test set.\")\n",
    "print(f\"{n_ignored_features} out of {len(la.coef_)} features were ignored.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining feature selectors\n",
    "\n",
    "In the previous lesson we saw how Lasso models allow you to tweak the strength of regularization with the alpha parameter. \n",
    "\n",
    "We manually set this alpha parameter to find a balance between removing as much features as possible and model accuracy. However, manually finding a good alpha value can be tedious. Good news is, there is a way to automate this. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LassoCV regressor\n",
    "\n",
    "The `LassoCV()` class will use cross validation to try out different alpha settings and select the best one. When we fit this model to our training data it will get an `alpha_` attribute with the optimal value. \n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "lcv = LassoCV()\n",
    "lcv.fit(X_train, y_train)\n",
    "\n",
    "print(lcv.alpha_)\n",
    "```\n",
    "\n",
    "```sh\n",
    "0.09\n",
    "```\n",
    "\n",
    "To actually remove the features to which the Lasso regressor assigned a zero coefficient, we once again create a mask with True values for all non-zero coefficients. We can then apply it to our feature dataset `X` with the `loc` method. \n",
    "\n",
    "```python\n",
    "mask = lcv.coef_ != 0\n",
    "\n",
    "reduced_X = X.loc[:, mask]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taking a step back\n",
    "\n",
    "* Random forest is a combination of decision trees (multiple weak predictors can combine to form a strong one)\n",
    "* We can use a combination of models for feature selection too"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection with LassoCV\n",
    "\n",
    "To do so lets first train the models one by one.\n",
    "\n",
    "We'll be predicting BMI in the ANSUR dataset just like you did in the last exercises. If we use `LassoCV()` we'll get an R squared of 99% and when we create a mask that tells us whether a feature has a non-zero coefficient we see that this is true for 66 out of 91 features. We'll put this `lcv_mask` to the side for a moment and move on to the next model.\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "lcv-LassoCV()\n",
    "lcv.fit(X_train, y_train)\n",
    "lcv.score(X_test, y_test)\n",
    "```\n",
    "\n",
    "```sh\n",
    "0.99\n",
    "```\n",
    "\n",
    "```python\n",
    "lcv_mask = lcv.coef_ != 0\n",
    "sum(lcv_mask)\n",
    "```\n",
    "\n",
    "```sh\n",
    "66\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection with random forest\n",
    "\n",
    "The second model we train is a random forest regressor model. We've wrapped a Recursive Feature Selector or RFE, around the model to have it select the same number of features as the `LassoCV()` regressor did. We can then use the `support_` attribute of the fitted model to create `rf_mask`. \n",
    "\n",
    "```python\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rfe_rf = RFE(estimator=RandomForestRegressor(), n_features_to_select=66, step=5, verbose=1)\n",
    "rfe_fr.fit(X_train, y_train)\n",
    "rf_mask = rfe_rf.support_\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection with gradient boosting\n",
    "\n",
    "Then, we do the same thing with a gradient boosting regressor. Like random forests gradient boosting is an ensemble method that will calculate feature importance values. The trained model too has a `support_` attribute that we can use to create `gb_mask`. \n",
    "\n",
    "```python\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "rfe_gb = RFE(estimator=GradientBoostingRegressor(), n_features_to_select=66, step=5, verbose=1)\n",
    "rfe_gb.fit(X_train, y_train)\n",
    "gb_mask = rfe_gb.support_\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining the feature selectors\n",
    "\n",
    "Finally, we can start counting the votes on whether to select a feature. We use NumPy's `sum()` function, pass it the three masks in a list, and set the `axis` argument to 0. \n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "votes = np.sum([lcv_mask, rf_mask, gb_mask], axis=0)\n",
    "print(votes)\n",
    "```\n",
    "\n",
    "We'll then get an array with the number of votes that each feature got.\n",
    "\n",
    "```sh\n",
    "array([3, 2, 2, ..., 3, 0, 1])\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "What we do with this vote depends on how conservative we want to be.\n",
    "\n",
    "If we want to make sure we don't lose any information, we could select all features with at least one vote.\n",
    "\n",
    "In this example we chose to have at least two models voting for a feature in order to keep it.\n",
    "\n",
    "```python\n",
    "mask = votes > 2\n",
    "```\n",
    "\n",
    "All that is left now is to actually implement the dimensionality reduction. We do that with the `loc` method of our feature DataFrame `X`. \n",
    "\n",
    "```python\n",
    "reduced_x = X.loc[:, mask]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a LassoCV regressor\n",
    "\n",
    "You'll be predicting biceps circumference on a subsample of the male ANSUR dataset using the `LassoCV()` regressor that automatically tunes the regularization strength (alpha value) using Cross-Validation.\n",
    "\n",
    "The standardized training and test data has been pre-loaded for you as `X_train`, `X_test`, `y_train`, and `y_test`.\n",
    "\n",
    "##### Instructions\n",
    "\n",
    "* Create and fit the LassoCV model on the training set.\n",
    "* Calculate $R^2$ on the test set.\n",
    "* Create a mask for coefficients not equal to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "ansur_df = pd.read_csv(\"data/ansur_lasso.csv\")\n",
    "\n",
    "y = ansur_df[\"biceps circumference flexed\"]\n",
    "X = ansur_df.drop(\"biceps circumference flexed\", axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal alpha = 2.171\n",
      "The model explains 88.1% of the test set variance\n",
      "20 features out of 32 selected\n"
     ]
    }
   ],
   "source": [
    "# create and fit the LassoCV model on the training set\n",
    "lcv = LassoCV(random_state=0)\n",
    "lcv.fit(X_train, y_train)\n",
    "print(f\"Optimal alpha = {lcv.alpha_:.3f}\")\n",
    "\n",
    "# calculate R squared on the test set\n",
    "r_squared = lcv.score(X_test, y_test)\n",
    "print(f\"The model explains {r_squared:.1%} of the test set variance\")\n",
    "\n",
    "# create a mask for coefficients not equal to zero\n",
    "lcv_mask = lcv.coef_ != 0\n",
    "print(f\"{sum(lcv_mask)} features out of {len(lcv_mask)} selected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features:\n",
      "acromialheight\n",
      "bideltoidbreadth\n",
      "buttockcircumference\n",
      "buttockkneelength\n",
      "buttockpopliteallength\n",
      "chestcircumference\n",
      "chestheight\n",
      "forearmcircumferenceflexed\n",
      "iliocristaleheight\n",
      "interscyeii\n",
      "lateralfemoralepicondyleheight\n",
      "lateralmalleolusheight\n",
      "radialestylionlength\n",
      "shouldercircumference\n",
      "shoulderelbowlength\n",
      "thighcircumference\n",
      "thighclearance\n",
      "verticaltrunkcircumferenceusa\n",
      "waistcircumference\n",
      "waistdepth\n"
     ]
    }
   ],
   "source": [
    "selected_features = sorted(X_train.columns[lcv_mask])\n",
    "\n",
    "print(\"Selected features:\")\n",
    "for feature in selected_features:\n",
    "    print(feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ensemble models for extra votes\n",
    "\n",
    "The `LassoCV()` model selected 22 out of 32 features. Not bad, but not a spectacular dimensionality reduction either. Let's use two more models to select the 10 features they consider most important using the Recursive Feature Eliminator (RFE).\n",
    "\n",
    "The standardized training and test data has been pre-loaded for you as `X_train`, `X_test`, `y_train`, and `y_test`.\n",
    "\n",
    "\n",
    "##### Instructions 1/4\n",
    "\n",
    "* Select 10 features with RFE on a `GradientBoostingRegressor` and drop 3 features on each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "ansur_df = pd.read_csv(\"data/ansur_lasso.csv\")\n",
    "\n",
    "y = ansur_df[\"biceps circumference flexed\"]\n",
    "X = ansur_df.drop(\"biceps circumference flexed\", axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 32 features.\n",
      "Fitting estimator with 29 features.\n",
      "Fitting estimator with 26 features.\n",
      "Fitting estimator with 23 features.\n",
      "Fitting estimator with 20 features.\n",
      "Fitting estimator with 17 features.\n",
      "Fitting estimator with 14 features.\n",
      "Fitting estimator with 11 features.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-3 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-3 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-3 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-3 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-3 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-3 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-3 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-3 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-3 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-3 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RFE(estimator=GradientBoostingRegressor(), n_features_to_select=10, step=3,\n",
       "    verbose=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;RFE<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.feature_selection.RFE.html\">?<span>Documentation for RFE</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>RFE(estimator=GradientBoostingRegressor(), n_features_to_select=10, step=3,\n",
       "    verbose=1)</pre></div> </div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">estimator: GradientBoostingRegressor</label><div class=\"sk-toggleable__content fitted\"><pre>GradientBoostingRegressor()</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" ><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;GradientBoostingRegressor<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html\">?<span>Documentation for GradientBoostingRegressor</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>GradientBoostingRegressor()</pre></div> </div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "RFE(estimator=GradientBoostingRegressor(), n_features_to_select=10, step=3,\n",
       "    verbose=1)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfe_gb = RFE(estimator=GradientBoostingRegressor(), n_features_to_select=10, step=3, verbose=1)\n",
    "rfe_gb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Instructions 2/4\n",
    "\n",
    "* Calculate the $R^2$ on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model can explain 85.6% of the variance in the test set\n"
     ]
    }
   ],
   "source": [
    "r_squared = rfe_gb.score(X_test, y_test)\n",
    "print(f\"The model can explain {r_squared:.1%} of the variance in the test set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Instructions 3/4\n",
    "\n",
    "* Assign the support array of the fitted model to `gb_mask`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_mask = rfe_gb.support_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Instructions 4/4\n",
    "\n",
    "* Modify the first step to select 10 features with RFE on a `RandomForestRegressor()` and drop 3 features on each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 32 features.\n",
      "Fitting estimator with 29 features.\n",
      "Fitting estimator with 26 features.\n",
      "Fitting estimator with 23 features.\n",
      "Fitting estimator with 20 features.\n",
      "Fitting estimator with 17 features.\n",
      "Fitting estimator with 14 features.\n",
      "Fitting estimator with 11 features.\n",
      "The model can explain 83.9% of the variance in the test set\n"
     ]
    }
   ],
   "source": [
    "# select 10 features with RFE on a RandomForestRegressor, drop 3 features on each step\n",
    "rfe_rf = RFE(estimator=RandomForestRegressor(), n_features_to_select=10, step=3, verbose=1)\n",
    "rfe_rf.fit(X_train, y_train)\n",
    "\n",
    "# Calculate the R squared on the test set\n",
    "r_squared = rfe_rf.score(X_test, y_test)\n",
    "print(f\"The model can explain {r_squared:.1%} of the variance in the test set\")\n",
    "\n",
    "# Assign the support array to rf_mask\n",
    "rf_mask = rfe_rf.support_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combining 3 feature selectors\n",
    "\n",
    "We'll combine the votes of the 3 models you built in the previous exercises, to decide which features are important into a meta mask. We'll then use this mask to reduce dimensionality and see how a simple linear regressor performs on the reduced dataset.\n",
    "\n",
    "The per model votes have been pre-loaded as `lcv_mask`, `rf_mask`, and `gb_mask` and the feature and target datasets as X and y.\n",
    "\n",
    "##### Instructions 1/4\n",
    "\n",
    "* Sum the votes of the three models using `np.sum()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 3 3 1 1 0 3 1 0 0 3 0 0 0 1 1 1 1 2 0 1 3 1 0 3 2 2 2 1 1 2]\n"
     ]
    }
   ],
   "source": [
    "votes = np.sum([lcv_mask, rf_mask, gb_mask], axis=0)\n",
    "print(votes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Instructions 2/4\n",
    "\n",
    "* Create a mask for features selected by all 3 models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False False  True  True False False False  True False False False  True\n",
      " False False False False False False False False False False  True False\n",
      " False  True False False False False False False]\n"
     ]
    }
   ],
   "source": [
    "meta_mask = votes == 3\n",
    "print(meta_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Instructions 3/4\n",
    "\n",
    "* Apply the dimensionality reduction on X and print which features were selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bideltoidbreadth', 'buttockcircumference', 'chestcircumference', 'forearmcircumferenceflexed', 'shouldercircumference', 'thighcircumference']\n"
     ]
    }
   ],
   "source": [
    "X_reduced = X.loc[:, meta_mask]\n",
    "print(sorted(X_reduced.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Instructions 4/4\n",
    "\n",
    "* Plug the reduced dataset into the code for simple linear regression that has been written for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "lm = LinearRegression()\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model can explain 86.6% of the variance in the test set using 6 features.\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_reduced, y, test_size=0.3, random_state=0)\n",
    "lm.fit(scaler.fit_transform(X_train), y_train)\n",
    "r_squared = lm.score(scaler.transform(X_test), y_test)\n",
    "print(f\"The model can explain {r_squared:.1%} of the variance in the test set using {len(lm.coef_)} features.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
