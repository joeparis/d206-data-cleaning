{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Data in Python\n",
    "\n",
    "It's commonly said that data scientists spend 80% of their time cleaning and manipulating data and only 20% of their time analyzing it. The time spent cleaning is vital since analyzing dirty data can lead you to draw inaccurate conclusions.\n",
    "\n",
    "Data cleaning is an essential task in data science. Without properly cleaned data, the results of any data analysis or machine learning model could be inaccurate. In this course, you will learn how to identify, diagnose, and treat a variety of data cleaning problems in Python, ranging from simple to advanced. You will deal with improper data types, check that your data is in the correct range, handle missing data, perform record linkage, and more!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common data problems\n",
    "\n",
    "In this chapter, you'll learn how to overcome some of the most common dirty data problems. You'll convert data types, apply range constraints to remove future data points, and remove duplicated data points to avoid double-counting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data type constraints\n",
    "\n",
    "Course outline:\n",
    "\n",
    "* diagnose dirty data\n",
    "* side effects of dirty data\n",
    "* cleaning data\n",
    "\n",
    "#### Why do we need to clean data?\n",
    "\n",
    "DS workflow:\n",
    "\n",
    "access data --> explore and process data --> extract insights --> report insights\n",
    "\n",
    "Dirty data is caused by both human and technical errors and will taint every step of the DS workflow the data is used in.\n",
    "\n",
    "GIGO\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numeric data or ... ?\n",
    "\n",
    "In this exercise, and throughout this chapter, you'll be working with bicycle ride sharing data in San Francisco called `ride_sharing`. It contains information on the start and end stations, the trip duration, and some user information for a bike sharing service.\n",
    "\n",
    "The `user_type` column contains information on whether a user is taking a free ride and takes on the following values:\n",
    "\n",
    "`1` for free riders.\n",
    "`2` for pay per ride.\n",
    "`3` for monthly subscribers.\n",
    "\n",
    "In this instance, you will print the information of `ride_sharing` using `.info()` and see a firsthand example of how an incorrect data type can flaw your analysis of the dataset. The pandas package is imported as `pd`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"set up\"\"\"\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "ride_sharing_csv = Path.cwd() / \"data/ride_sharing_new.csv\"\n",
    "\n",
    "ride_sharing = pd.read_csv(ride_sharing_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Instructions 1/3\n",
    "\n",
    "* Print the information of `ride_sharing`.\n",
    "* Use `.describe()` to print the summary statistics of the `user_type` column from `ride_sharing`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 25760 entries, 0 to 25759\n",
      "Data columns (total 10 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   Unnamed: 0       25760 non-null  int64 \n",
      " 1   duration         25760 non-null  object\n",
      " 2   station_A_id     25760 non-null  int64 \n",
      " 3   station_A_name   25760 non-null  object\n",
      " 4   station_B_id     25760 non-null  int64 \n",
      " 5   station_B_name   25760 non-null  object\n",
      " 6   bike_id          25760 non-null  int64 \n",
      " 7   user_type        25760 non-null  int64 \n",
      " 8   user_birth_year  25760 non-null  int64 \n",
      " 9   user_gender      25760 non-null  object\n",
      "dtypes: int64(6), object(4)\n",
      "memory usage: 2.0+ MB\n",
      "None\n",
      "count    25760.000000\n",
      "mean         2.008385\n",
      "std          0.704541\n",
      "min          1.000000\n",
      "25%          2.000000\n",
      "50%          2.000000\n",
      "75%          3.000000\n",
      "max          3.000000\n",
      "Name: user_type, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Print the information of ride_sharing\n",
    "print(ride_sharing.info())\n",
    "\n",
    "# Print summary statistics of user_type column\n",
    "print(ride_sharing[\"user_type\"].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Instructions 2/3\n",
    "\n",
    "By looking at the summary statistics - they don't really seem to offer much description on how users are distributed along their purchase type, why do you think that is?\n",
    "\n",
    "**Possible answers**\n",
    "\n",
    "1. The `user_type` column is not of the correct type, it should be converted to `str`.\n",
    "2. The `user_type` column has an infinite set of possible values, it should be converted to `category`.\n",
    "3. The `user_type` column has an finite set of possible values that represent groupings of data, it should be converted to `category`.\n",
    "\n",
    "**Answer: 3**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Instructions 3/3\n",
    "\n",
    "* Convert `user_type` into categorical by assigning it the `'category'` data type and store it in the `user_type_cat` column.\n",
    "* Make sure you converted `user_type_cat` correctly by using an `assert` statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 25760 entries, 0 to 25759\n",
      "Data columns (total 10 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   Unnamed: 0       25760 non-null  int64 \n",
      " 1   duration         25760 non-null  object\n",
      " 2   station_A_id     25760 non-null  int64 \n",
      " 3   station_A_name   25760 non-null  object\n",
      " 4   station_B_id     25760 non-null  int64 \n",
      " 5   station_B_name   25760 non-null  object\n",
      " 6   bike_id          25760 non-null  int64 \n",
      " 7   user_type        25760 non-null  int64 \n",
      " 8   user_birth_year  25760 non-null  int64 \n",
      " 9   user_gender      25760 non-null  object\n",
      "dtypes: int64(6), object(4)\n",
      "memory usage: 2.0+ MB\n",
      "None\n",
      "count    25760.000000\n",
      "mean         2.008385\n",
      "std          0.704541\n",
      "min          1.000000\n",
      "25%          2.000000\n",
      "50%          2.000000\n",
      "75%          3.000000\n",
      "max          3.000000\n",
      "Name: user_type, dtype: float64\n",
      "count     25760\n",
      "unique        3\n",
      "top           2\n",
      "freq      12972\n",
      "Name: user_type_cat, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Print the information of ride_sharing\n",
    "print(ride_sharing.info())\n",
    "\n",
    "# Print summary statistics of user_type column\n",
    "print(ride_sharing[\"user_type\"].describe())\n",
    "\n",
    "# Convert user_type from integer to category\n",
    "ride_sharing[\"user_type_cat\"] = ride_sharing[\"user_type\"].astype(\"category\")\n",
    "\n",
    "# Write an assert statement confirming the change\n",
    "assert ride_sharing[\"user_type_cat\"].dtype == \"category\"\n",
    "\n",
    "# Print new summary statistics\n",
    "print(ride_sharing[\"user_type_cat\"].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summing strings and concatenating numbers\n",
    "\n",
    "In the previous exercise, you were able to identify that `category` is the correct data type for `user_type` and convert it in order to extract relevant statistical summaries that shed light on the distribution of `user_type`.\n",
    "\n",
    "Another common data type problem is importing what should be numerical values as strings, as mathematical operations such as summing and multiplication lead to string concatenation, not numerical outputs.\n",
    "\n",
    "In this exercise, you'll be converting the string column `duration` to the type `int`. Before that however, you will need to make sure to strip `\"minutes\"` from the column in order to make sure pandas reads it as numerical. The pandas package has been imported as `pd`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         duration duration_trim  duration_time\n",
      "0      12 minutes           12              12\n",
      "1      24 minutes           24              24\n",
      "2       8 minutes            8               8\n",
      "3       4 minutes            4               4\n",
      "4      11 minutes           11              11\n",
      "...           ...           ...            ...\n",
      "25755  11 minutes           11              11\n",
      "25756  10 minutes           10              10\n",
      "25757  14 minutes           14              14\n",
      "25758  14 minutes           14              14\n",
      "25759  29 minutes           29              29\n",
      "\n",
      "[25760 rows x 3 columns]\n",
      "11.389052795031056\n"
     ]
    }
   ],
   "source": [
    "# Strip duration of minutes\n",
    "ride_sharing[\"duration_trim\"] = ride_sharing[\"duration\"].str.strip(\"minutes\")\n",
    "\n",
    "# Convert duration to integer\n",
    "ride_sharing[\"duration_time\"] = ride_sharing[\"duration_trim\"].astype(\"int\")\n",
    "\n",
    "# Write an assert statement making sure of conversion\n",
    "assert ride_sharing[\"duration_time\"].dtype == \"int\"\n",
    "\n",
    "# Print formed columns and calculate average ride duration\n",
    "print(ride_sharing[[\"duration\", \"duration_trim\", \"duration_time\"]])\n",
    "print(ride_sharing[\"duration_time\"].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data range constraints\n",
    "\n",
    "Examples of constraints being violated include a movie having a rating of 6 on a scale of 1-5 or subscription dates that are in the future.\n",
    "\n",
    "```python\n",
    "import datetime as dt\n",
    "user_signups[user_signups['subscription_date] > dt.date.today()]\n",
    "```\n",
    "\n",
    "#### How to deal with out of range data?\n",
    "\n",
    "* drop it (caution!)\n",
    "* setting custom minimums and maximums\n",
    "* treat as missing and impute\n",
    "* setting custom value depending on business assumptions\n",
    "\n",
    "#### Movies example\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "# output movies with rating > 5\n",
    "movies[movies['avg_rating'] > 5]\n",
    "\n",
    "# drop values using filtering\n",
    "movies = movies[movies['avg_rating'] <= 5]\n",
    "# drop values using .drop()\n",
    "movies.drop(movies[movies['avg_rating'] > 5].index, inplace = True)\n",
    "# assert results\n",
    "assert movies['avg_rating'].max() <= 5\n",
    "```\n",
    "\n",
    "```python\n",
    "# convert avg_rating > 5 to 5\n",
    "movies.loc[movies['avg_rating'] > 5, 'avg_rating'] = 5\n",
    "# assert results\n",
    "assert movies['avg_rating'].max() <= 5\n",
    "```\n",
    "\n",
    "#### Date range example\n",
    "\n",
    "```python\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "\n",
    "# output data types\n",
    "user_signups.dtypes  # subscription_date is 'object'\n",
    "# convert to date\n",
    "user_signups['subscription_date'] = pd.to_datetime(user_signups['subscription_date']).dt.date\n",
    "\n",
    "today_date = dt.date.today()\n",
    "\n",
    "## drop the data\n",
    "# drop values using filtering\n",
    "user_signups = user_signups[user_signups['subscription_date'] < today_date]\n",
    "# drop values using .drop()\n",
    "user_signups.drop(user_signups[user_signups['subsctiption_date'] > today_date].index, inplace = True)\n",
    "\n",
    "## Hardcode dates with upper limit\n",
    "user_signups.loc[user_signups['subscription_date'] > today_date, 'subscription_date'] = today_date\n",
    "assert user_signups.subscription_date.max().date() <= today_date\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count     25760\n",
      "unique        2\n",
      "top          27\n",
      "freq      17241\n",
      "Name: tire_sizes, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# The ride_sharing DataFrame provided does not have the tire_sizes column so we\n",
    "# must first create it.\n",
    "\n",
    "sizes = [26, 27, 29]\n",
    "ride_sharing[\"tire_sizes\"] = np.random.choice(sizes, len(ride_sharing))\n",
    "ride_sharing[\"tire_sizes\"] = ride_sharing[\"tire_sizes\"].astype(\"category\")\n",
    "\n",
    "# Convert tire_sizes to integer\n",
    "ride_sharing[\"tire_sizes\"] = ride_sharing[\"tire_sizes\"].astype(\"int\")\n",
    "\n",
    "# Set all values above 27 to 27\n",
    "ride_sharing.loc[ride_sharing.tire_sizes > 27, \"tire_sizes\"] = 27\n",
    "\n",
    "# Reconvert tire_sizes back to categorical\n",
    "ride_sharing[\"tire_sizes\"] = ride_sharing[\"tire_sizes\"].astype(\"category\")\n",
    "\n",
    "# Print tire size description\n",
    "print(ride_sharing[\"tire_sizes\"].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tire size constraints\n",
    "\n",
    "In this lesson, you're going to build on top of the work you've been doing with the `ride_sharing` DataFrame. You'll be working with the `tire_sizes` column which contains data on each bike's tire size.\n",
    "\n",
    "Bicycle tire sizes could be either 26″, 27″ or 29″ and are here correctly stored as a categorical value. In an effort to cut maintenance costs, the ride sharing provider decided to set the maximum tire size to be 27″.\n",
    "\n",
    "In this exercise, you will make sure the `tire_sizes` column has the correct range by first converting it to an integer, then setting and testing the new upper limit of 27″ for tire sizes.\n",
    "\n",
    "##### Instructions\n",
    "\n",
    "* Convert the `tire_sizes` column from `category` to '`int`'.\n",
    "* Use `.loc[]` to set all values of `tire_sizes` above 27 to 27.\n",
    "* Reconvert back `tire_sizes` to `'category'` from `int`.\n",
    "* Print the description of the `tire_sizes`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert tire_sizes to integer\n",
    "ride_sharing[\"tire_sizes\"] = ride_sharing[\"tire_sizes\"].astype(\"int\")\n",
    "\n",
    "# Set all values above 27 to 27\n",
    "ride_sharing.loc[ride_sharing.tire_sizes > 27, \"tire_sizes\"] = 27\n",
    "\n",
    "# Reconvert tire_sizes back to categorical\n",
    "ride_sharing[\"tire_sizes\"] = ride_sharing[\"tire_sizes\"].astype(\"category\")\n",
    "\n",
    "# Print tire size description\n",
    "print(ride_sharing[\"tire_sizes\"].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back to the future\n",
    "\n",
    "A new update to the data pipeline feeding into the `ride_sharing` DataFrame has been updated to register each ride's date. This information is stored in the `ride_date` column of the type `object`, which represents strings in `pandas`.\n",
    "\n",
    "A bug was discovered which was relaying rides taken today as taken next year. To fix this, you will find all instances of the `ride_date` column that occur anytime in the future, and set the maximum possible value of this column to today's date. Before doing so, you would need to convert `ride_date` to a `datetime` object.\n",
    "\n",
    "The `datetime` package has been imported as `dt`, alongside all the packages you've been using till now.\n",
    "\n",
    "##### Instructions\n",
    "\n",
    "\n",
    "* Convert `ride_date` to a `datetime` object using `to_datetime()`, then convert the `datetime` object into a `date` and store it in `ride_dt` column.\n",
    "* Create the variable `today`, which stores today's date by using the `dt.date.today()` function.\n",
    "* For all instances of `ride_dt` in the future, set them to today's date.\n",
    "* Print the maximum date in the `ride_dt` column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "\n",
    "# The ride_sharing DataFrame provided does not have the ride_date column so we\n",
    "# must first create it.\n",
    "\n",
    "\n",
    "def random_dates(start: pd.Timestamp, end: pd.Timestamp, n=1):\n",
    "    \"\"\"Retrun n random dates between start and end..\"\"\"\n",
    "    # start_date = start.value // (10**9)\n",
    "    # end_date = end.value // (10**9)\n",
    "    start_date = start.value // (24 * 60 * 60 * 10**9)\n",
    "    end_date = end.value // (24 * 60 * 60 * 10**9)\n",
    "    return pd.to_datetime(np.random.randint(start_date, end_date, n), unit=\"D\")\n",
    "\n",
    "\n",
    "start_date = pd.to_datetime(\"2017-01-01\")\n",
    "end_date = pd.to_datetime(\"2025-12-31\")\n",
    "dates = random_dates(start_date, end_date, len(ride_sharing))\n",
    "ride_sharing[\"ride_date\"] = dates\n",
    "\n",
    "# Convert ride_date to date\n",
    "ride_sharing[\"ride_dt\"] = pd.to_datetime(ride_sharing[\"ride_date\"]).dt.date\n",
    "\n",
    "# Save today's date\n",
    "today = dt.date.today()\n",
    "\n",
    "# Set all in the future to today's date\n",
    "ride_sharing.loc[ride_sharing[\"ride_dt\"] > today, \"ride_dt\"] = today\n",
    "\n",
    "# Print maximum of ride_dt column\n",
    "print(ride_sharing[\"ride_dt\"].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uniqueness constraints\n",
    "\n",
    "#### How to find duplicate values?\n",
    "\n",
    "Use the `.duplicated()` method. Returns a Series of bools.\n",
    "\n",
    "``` python\n",
    "duplicates = height_weight.duplicated()\n",
    "print(duplicates)\n",
    "```\n",
    "\n",
    "``` text\n",
    "1       False\n",
    "...     ...\n",
    "22      True\n",
    "23      False\n",
    "...     ...\n",
    "```\n",
    "\n",
    "See which rows are duplicated. \n",
    "\n",
    "Problem is that all duplicated values are marked *except* the first one which makes it hard to know exactly what type of duplication we have (the results below show the second and subsequent occurrence of whatever duplicate value was found).\n",
    "\n",
    "``` python\n",
    "duplicates = height_weight.duplicated()\n",
    "height_weight[duplicates]\n",
    "```\n",
    "\n",
    "``` text\n",
    "    first_name last_name                            address height weight\n",
    "100       Mary     Colon                        4674 Ut Rd.    179     75\n",
    "101       Ivor    Pierce                  102-3364 Non Road    168     88\n",
    "102       Cole    Palmer                    8366 At, Street    178     91\n",
    "103    Desirae   Shannon PO Box 643, 5251 Consectetuer, Rd.    196     83\n",
    "```\n",
    "\n",
    "To fix this, use these two arguments to `duplicated()`:\n",
    "\n",
    "* `subset`: List of column names to check for duplication\n",
    "* `keep`: Whether to keep **first** (`first`), **last** (`last`), or **all** (`False`) duplicate values.\n",
    "\n",
    "Here we are checking for duplicates across the first name, last name, and address variables and we're choosing to keep all the duplicates.\n",
    "\n",
    "``` python\n",
    "column_names = ['first_name', 'last_name', 'address']\n",
    "duplicates = height_weight.duplicated(subset = column_names, keep = False)\n",
    "```\n",
    "\n",
    "``` text\n",
    "    first_name last_name                            address height weight\n",
    "1         Ivor    Pierce                  102-3364 Non Road    168     66\n",
    "22        Cole    Palmer                    8366 At, Street    178     91\n",
    "28     Desirae   Shannon PO Box 643, 5251 Consectetuer, Rd.    195     83\n",
    "37        Mary     Colon                        4674 Ut Rd.    179     75\n",
    "100       Mary     Colon                        4674 Ut Rd.    179     75\n",
    "101       Ivor    Pierce                  102-3364 Non Road    168     88\n",
    "102       Cole    Palmer                    8366 At, Street    178     91\n",
    "103    Desirae   Shannon PO Box 643, 5251 Consectetuer, Rd.    196     83\n",
    "```\n",
    "\n",
    "``` python\n",
    "height_weight[duplicates].sort_values(by = 'first_name')\n",
    "```\n",
    "\n",
    "``` text\n",
    "    first_name last_name                            address height weight\n",
    "22        Cole    Palmer                    8366 At, Street    178     91\n",
    "102       Cole    Palmer                    8366 At, Street    178     91\n",
    "28     Desirae   Shannon PO Box 643, 5251 Consectetuer, Rd.    195     83\n",
    "103    Desirae   Shannon PO Box 643, 5251 Consectetuer, Rd.    196     83\n",
    "1         Ivor    Pierce                  102-3364 Non Road    168     66\n",
    "101       Ivor    Pierce                  102-3364 Non Road    168     88\n",
    "37        Mary     Colon                        4674 Ut Rd.    179     75\n",
    "100       Mary     Colon                        4674 Ut Rd.    179     75\n",
    "```\n",
    "\n",
    "We find that there are four sets of duplicated rows, the first 2 (Cole Palmer) and last 2 (Mary Colon) being complete duplicates of each other across all columns.\n",
    "\n",
    "The other 2 (Desirae Shannon and Ivor Pierce) being incomplete duplicates of each other with discrepancies across height and weight respectively. \n",
    "\n",
    "#### How to treat duplicate values?\n",
    "\n",
    "Complete duplicates are easy. Use the `.drop_duplicates()` method to which also takes in the same `subset` and `keep` arguments as in the `duplicated()` method, as well as the `inplace` argument which drops the duplicated values directly inside the height_weight DataFrame. \n",
    "\n",
    "* `subset`: List of column names to check for duplication\n",
    "* `keep`: Whether to keep **first** (`first`), **last** (`last`), or **all** (`False`) duplicate values.\n",
    "* `inplace`: Drop duplicated rows directly inside DataFrame without crating new object.\n",
    "\n",
    "``` python\n",
    "# drop complete duplicates only\n",
    "height_weight.drop_duplicates(inplace = True)\n",
    "```\n",
    "\n",
    "Since the `keep` argument takes in `first` as default, we can keep it as such. Note that we can also set it as `last`, but not as `False` as it would keep all duplicates. \n",
    "\n",
    "This leaves the other 2 sets of duplicates which have the same `first_name`, `last_name`, and `address` but contain discrepancies in `height` and `weight`. Apart from dropping rows with really small discrepancies, we can use a statistical measure to combine each set of duplicated values. \n",
    "\n",
    "For example, we can combine these two rows into one by computing the average mean between them, or the maximum, or other statistical measures, this is highly dependent on a common sense understanding of our data, and what type of data we have. \n",
    "\n",
    "``` python\n",
    "column_names = ['first_name', 'last_name', 'address']\n",
    "duplicates = height_weight.duplicated(subset = column_names, keep = False)\n",
    "height_weight[duplicates].sort_values(by = 'first_name')\n",
    "```\n",
    "\n",
    "``` text\n",
    "    first_name last_name                            address height weight\n",
    "28     Desirae   Shannon PO Box 643, 5251 Consectetuer, Rd.    195     83\n",
    "103    Desirae   Shannon PO Box 643, 5251 Consectetuer, Rd.    196     83\n",
    "1         Ivor    Pierce                  102-3364 Non Road    168     66\n",
    "101       Ivor    Pierce                  102-3364 Non Road    168     88\n",
    "```\n",
    "\n",
    "Do this using the `.groupby()` and `.agg()` methods which let you group by a set of common columns and return statistical values for specific columns when the aggregation is being performed. \n",
    "\n",
    "For example here, we created a dictionary called summaries, which instructs `groupby` to return the maximum of duplicated rows for the height column, and the mean duplicated rows for the weight column. We then group `height_weight` by the column names defined earlier, and chained it with the `agg` method, which takes in the summaries dictionary we created. We chain this entire line with the `.reset_index()` method, so that we can have numbered indices in the final output. We can verify that there are no more duplicate values by running the duplicated method again, and use brackets to output duplicate rows. \n",
    "\n",
    "``` python\n",
    "# group by column names and produce statistical summaries\n",
    "column_names = ['first_name', 'last_name', 'address']\n",
    "summaries = {'height': 'max', 'weight': 'mean'}\n",
    "height_weight = height_weight.groupby(by = column_names).agg(summaries).reset_index()\n",
    "# verify no more duplicates\n",
    "duplicates = height_weight.duplicated(subset = colum_names, keep = False)\n",
    "height_weight[duplicates].sort_values(by = 'first_name')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding duplicates\n",
    "\n",
    "A new update to the data pipeline feeding into `ride_sharing` has added the `ride_id` column, which represents a unique identifier for each ride.\n",
    "\n",
    "The update however coincided with radically shorter average ride duration times and irregular user birth dates set in the future. Most importantly, the number of rides taken has increased by 20% overnight, leading you to think there might be both complete and incomplete duplicates in the `ride_sharing` DataFrame.\n",
    "\n",
    "In this exercise, you will confirm this suspicion by finding those duplicates. A sample of `ride_sharing` is in your environment, as well as all the packages you've been working with thus far.\n",
    "\n",
    "##### Instructions\n",
    "\n",
    "* Find duplicated rows of `ride_id` in the `ride_sharing` DataFrame while setting keep to False.\n",
    "* Subset `ride_sharing` on `duplicates` and sort by `ride_id` and assign the results to `duplicated_rides`.\n",
    "* Print the `ride_id`, `duration` and `user_birth_year` columns of `duplicated_rides` in that order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find duplicates\n",
    "duplicates = ride_sharing.duplicated(subset=\"ride_id\", keep=False)\n",
    "\n",
    "# Sort your duplicated rides\n",
    "duplicated_rides = ride_sharing[duplicates].sort_values(by=\"ride_id\")\n",
    "\n",
    "# Print relevant columns\n",
    "print(duplicated_rides[[\"ride_id\", \"duration\", \"user_birth_year\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treating duplicates\n",
    "\n",
    "In the last exercise, you were able to verify that the new update feeding into `ride_sharing` contains a bug generating both complete and incomplete duplicated rows for some values of the `ride_id` column, with occasional discrepant values for the `user_birth_year` and `duration` columns.\n",
    "\n",
    "In this exercise, you will be treating those duplicated rows by first dropping complete duplicates, and then merging the incomplete duplicate rows into one while keeping the average `duration`, and the minimum `user_birth_year` for each set of incomplete duplicate rows.\n",
    "\n",
    "##### Instructions\n",
    "\n",
    "* Drop complete duplicates in `ride_sharing` and store the results in `ride_dup`.\n",
    "* Create the statistics dictionary which holds **min**imum aggregation for `user_birth_year` and **mean** aggregation for `duration`.\n",
    "* Drop incomplete duplicates by grouping by `ride_id` and applying the aggregation in `statistics`.\n",
    "* Find duplicates again and run the `assert` statement to verify de-duplication.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop complete duplicates from ride_sharing\n",
    "ride_dup = ride_sharing.drop_duplicates()\n",
    "\n",
    "# Create statistics dictionary for aggregation function\n",
    "statistics = {\"user_birth_year\": \"min\", \"duration\": \"mean\"}\n",
    "\n",
    "# Group by ride_id and compute new statistics\n",
    "ride_unique = ride_dup.groupby(\"ride_id\").agg(statistics).reset_index()\n",
    "\n",
    "# Find duplicated values again\n",
    "duplicates = ride_unique.duplicated(subset=\"ride_id\", keep=False)\n",
    "duplicated_rides = ride_unique[duplicates == True]\n",
    "\n",
    "# Assert duplicates are processed\n",
    "assert duplicated_rides.shape[0] == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text and categorical data problems\n",
    "\n",
    "Categorical and text data can often be some of the messiest parts of a dataset due to their unstructured nature. In this chapter, you’ll learn how to fix whitespace and capitalization inconsistencies in category labels, collapse multiple categories into one, and reformat strings for consistency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Membership constraints\n",
    "\n",
    "Categorical variables represent predefined, finite sets of categories.\n",
    "\n",
    "| Type if data | Example values | Numeric (categorical) representation |\n",
    "|--------------|----------------|--------------------------------------|\n",
    "| Marriage Status | `unmarried`, `married` | `0`, `1` |\n",
    "| Household Income Category | `0-20K`, `20-40K`, ... | `0`, `1`, --- |\n",
    "| Loan Status | `default`, `payed`, `no_loan` | `0`, `1`, `2` |\n",
    "\n",
    "Machine learning models often require categorical data to be encoded as numbers.\n",
    "\n",
    "Since categorical data represent a predefined set of categories they can't have values that fall outside of the predefined categories.\n",
    "\n",
    "There are many reasons why there may be errors in our categorical data:\n",
    "\n",
    "* data entry errors\n",
    "* parsing errors\n",
    "\n",
    "There are a variety of ways these errors can be treated:\n",
    "\n",
    "* dropping data\n",
    "* remapping categories\n",
    "* inferring categories\n",
    "* ...\n",
    "\n",
    "Here's a DataFrame named `study_data` containing a list of first names, birth dates, and blood types. Additionally, a DataFrame named `categories`, containing the correct possible categories for the blood type column has been created as well. \n",
    "\n",
    "``` python\n",
    "# read study data and print it\n",
    "study_data = pd.read_csv('study.csv')\n",
    "study_data\n",
    "```\n",
    "\n",
    "``` text\n",
    "      name   birthday blood_type\n",
    "1     Beth 2019-10-20         B-\n",
    "2 Ignatius 2020-07-08         A-\n",
    "3     Paul 2019-08-12         O+\n",
    "4    Helen 2019-03-17         O-\n",
    "5 Jennifer 2019-12-17         Z+   <---\n",
    "6  Kennedy 2020-04-27         A+\n",
    "7    Keith 2019-04-19        AB+\n",
    "```\n",
    "\n",
    "``` python\n",
    "# correct possible blood types\n",
    "categories\n",
    "```\n",
    "\n",
    "``` text\n",
    "  blood_type\n",
    "1         O-\n",
    "2         O+\n",
    "3         A-\n",
    "4         A+\n",
    "5         B+\n",
    "6         B-\n",
    "7        AB+\n",
    "8        AB-\n",
    "```\n",
    "\n",
    "Notice the Z+ blood type. The `categories` DataFrame will help systematically spot rows with these inconsistencies. \n",
    "\n",
    "It's good practice to keep a log of all possible values of your categorical data.\n",
    "\n",
    "#### A note on joins\n",
    "\n",
    "The two main types of joins we care about here are **anti joins** and **inner joins**.\n",
    "\n",
    "Anti joins take in two DataFrames A and B, and return data from one DataFrame that is not contained in another. In this example, we are performing a left anti join of A and B, and are returning the columns of DataFrames A and B for values only found in A of the common column between them being joined on.\n",
    "\n",
    "Inner joins, return only the data that is contained in both DataFrames. For example, an inner join of A and B, would return columns from both DataFrames for values only found in A and B, of the common column between them being joined on. \n",
    "\n",
    "![anti joins and inner joins](images/joins_small.png)\n",
    "\n",
    "#### Left anti join on blood types\n",
    "\n",
    "In our example, an left anti join essentially returns all the data in study data with inconsistent blood types,\n",
    "\n",
    "![anti join](images/anti_join.png)\n",
    "\n",
    "and an inner join returns all the rows containing consistent blood types signs. \n",
    "\n",
    "![inner join](images/inner_join.png)\n",
    "\n",
    "#### Finding inconsistent categories\n",
    "\n",
    "``` python\n",
    "inconsistent_categories = set(study_data['blood_type']).difference(categories['blood_type'])\n",
    "print(inconsistent_categories)\n",
    "```\n",
    "\n",
    "``` text\n",
    "{'Z+}\n",
    "```\n",
    "\n",
    "``` python\n",
    "# get inconsistent rows\n",
    "inconsistent_rows = study_data['blood_type'].isin(inconsistent_categories)\n",
    "study_data[inconsistent_rows]\n",
    "```\n",
    "\n",
    "``` text\n",
    "      name   birthday blood_type\n",
    "5 Jennifer 2019-12-17         Z+\n",
    "```\n",
    "\n",
    "#### Dropping inconsistent categories\n",
    "\n",
    "Use the tilde symbol (`~`) while subsetting which returns everything except inconsistent rows\n",
    "\n",
    "``` python\n",
    "inconsistent_categories = set(study_data['blood_type']).difference(categories['blood_type'])\n",
    "inconsistent_rows = study_data['blood_type'].isin(inconsistent_categories)\n",
    "inconsistent_data = study_data[inconsistent_rows]\n",
    "# drop inconsistent categories and get consistent data only\n",
    "consistent_data = study_data[~inconsistent_rows]\n",
    "```\n",
    "\n",
    "``` text\n",
    "      name   birthday blood_type\n",
    "1     Beth 2019-10-20         B-\n",
    "2 Ignatius 2020-07-08         A-\n",
    "3     Paul 2019-08-12         O+\n",
    "4    Helen 2019-03-17         O-\n",
    "6  Kennedy 2020-04-27         A+\n",
    "7    Keith 2019-04-19        AB+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding consistency\n",
    "\n",
    "In this exercise and throughout this chapter, you'll be working with the `airlines` DataFrame which contains survey responses on the San Francisco Airport from airline customers.\n",
    "\n",
    "The DataFrame contains flight metadata such as the airline, the destination, waiting times as well as answers to key questions regarding cleanliness, safety, and satisfaction. Another DataFrame named `categories` was created, containing all correct possible values for the survey columns.\n",
    "\n",
    "In this exercise, you will use both of these DataFrames to find survey answers with inconsistent values, and drop them, effectively performing an outer and inner join on both these DataFrames as seen in the video exercise. The pandas package has been imported as pd, and the `airlines` and `categories` DataFrames are in your environment.\n",
    "\n",
    "##### Instructions 1/4\n",
    "\n",
    "* Print the `categories` DataFrame and take a close look at all possible correct categories of the survey columns.\n",
    "* Print the unique values of the survey columns in `airlines` using the `.unique()` method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "airlines_csv = Path.cwd() / \"data/airlines.csv\"\n",
    "categories_csv = Path.cwd() / \"data/airlines_categories.csv\"\n",
    "\n",
    "airlines = pd.read_csv(airlines_csv)\n",
    "categories = pd.read_csv(categories_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0     cleanliness           safety          satisfaction\n",
      "0           0           Clean          Neutral        Very satisfied\n",
      "1           1         Average        Very safe               Neutral\n",
      "2           2  Somewhat clean    Somewhat safe    Somewhat satisfied\n",
      "3           3  Somewhat dirty      Very unsafe  Somewhat unsatisfied\n",
      "4           4           Dirty  Somewhat unsafe      Very unsatisfied\n",
      "Cleanliness:  ['Clean' 'Average' 'Unacceptable' 'Somewhat clean' 'Somewhat dirty'\n",
      " 'Dirty'] \n",
      "\n",
      "Safety:  ['Neutral' 'Very safe' 'Somewhat safe' 'Very unsafe' 'Somewhat unsafe'] \n",
      "\n",
      "Satisfaction:  ['Very satisfied' 'Neutral' 'Somewhat satsified' 'Somewhat unsatisfied'\n",
      " 'Very unsatisfied'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print categories DataFrame\n",
    "print(categories)\n",
    "\n",
    "# Print unique values of survey columns in airlines\n",
    "print(\"Cleanliness: \", airlines[\"cleanliness\"].unique(), \"\\n\")\n",
    "print(\"Safety: \", airlines[\"safety\"].unique(), \"\\n\")\n",
    "print(\"Satisfaction: \", airlines[\"satisfaction\"].unique(), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Instructions 2/4\n",
    "\n",
    "**Question**\n",
    "\n",
    "Take a look at the output. Out of the `cleanliness`, `safety` and `satisfaction` columns **in the `airlines` DataFrame**, which one has an inconsistent category and what is it?\n",
    "\n",
    "**Possible answers**\n",
    "\n",
    "* `cleanliness` because it has an `Unacceptable` category.\n",
    "* `cleanliness` because it has a `Terribly dirty` category.\n",
    "* `satisfaction` because it has a `Very satisfied` category.\n",
    "* `safety` because it has a `Neutral` category.\n",
    "\n",
    "**Correct answer**\n",
    "\n",
    "`cleanliness` because it has an `Unacceptable` category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Instructions 3/4\n",
    "\n",
    "* Create a set out of the `cleanliness` column in `airlines` using `set()` and find the inconsistent category by finding the **difference** in the `cleanliness` column of `categories`.\n",
    "* Find rows of `airlines` with a `cleanliness` value not in `categories` and print the output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Unnamed: 0    id        day           airline  destination  dest_region  \\\n",
      "4            4  2992  Wednesday          AMERICAN        MIAMI      East US   \n",
      "17          18  2913     Friday  TURKISH AIRLINES     ISTANBUL  Middle East   \n",
      "89         100  2321  Wednesday         SOUTHWEST  LOS ANGELES      West US   \n",
      "\n",
      "   dest_size boarding_area   dept_time  wait_min   cleanliness         safety  \\\n",
      "4        Hub   Gates 50-59  2018-12-31     559.0  Unacceptable      Very safe   \n",
      "17       Hub  Gates 91-102  2018-12-31     225.0  Unacceptable      Very safe   \n",
      "89       Hub   Gates 20-39  2018-12-31     130.0  Unacceptable  Somewhat safe   \n",
      "\n",
      "          satisfaction  \n",
      "4   Somewhat satsified  \n",
      "17  Somewhat satsified  \n",
      "89  Somewhat satsified  \n"
     ]
    }
   ],
   "source": [
    "# Find the cleanliness category in airlines not in categories\n",
    "cat_clean = set(airlines[\"cleanliness\"]).difference(categories[\"cleanliness\"])\n",
    "\n",
    "# Find rows with that category\n",
    "cat_clean_rows = airlines[\"cleanliness\"].isin(cat_clean)\n",
    "\n",
    "# Print rows with inconsistent category\n",
    "print(airlines[cat_clean_rows])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Instructions 4/4\n",
    "\n",
    "* Print the rows with the consistent categories of `cleanliness` only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Unnamed: 0    id        day           airline  destination  dest_region  \\\n",
      "4            4  2992  Wednesday          AMERICAN        MIAMI      East US   \n",
      "17          18  2913     Friday  TURKISH AIRLINES     ISTANBUL  Middle East   \n",
      "89         100  2321  Wednesday         SOUTHWEST  LOS ANGELES      West US   \n",
      "\n",
      "   dest_size boarding_area   dept_time  wait_min   cleanliness         safety  \\\n",
      "4        Hub   Gates 50-59  2018-12-31     559.0  Unacceptable      Very safe   \n",
      "17       Hub  Gates 91-102  2018-12-31     225.0  Unacceptable      Very safe   \n",
      "89       Hub   Gates 20-39  2018-12-31     130.0  Unacceptable  Somewhat safe   \n",
      "\n",
      "          satisfaction  \n",
      "4   Somewhat satsified  \n",
      "17  Somewhat satsified  \n",
      "89  Somewhat satsified  \n",
      "      Unnamed: 0    id       day        airline        destination  \\\n",
      "0              0  1351   Tuesday    UNITED INTL             KANSAI   \n",
      "1              1   373    Friday         ALASKA  SAN JOSE DEL CABO   \n",
      "2              2  2820  Thursday          DELTA        LOS ANGELES   \n",
      "3              3  1157   Tuesday      SOUTHWEST        LOS ANGELES   \n",
      "5              5   634  Thursday         ALASKA             NEWARK   \n",
      "...          ...   ...       ...            ...                ...   \n",
      "2472        2804  1475   Tuesday         ALASKA       NEW YORK-JFK   \n",
      "2473        2805  2222  Thursday      SOUTHWEST            PHOENIX   \n",
      "2474        2806  2684    Friday         UNITED            ORLANDO   \n",
      "2475        2807  2549   Tuesday        JETBLUE         LONG BEACH   \n",
      "2476        2808  2162  Saturday  CHINA EASTERN            QINGDAO   \n",
      "\n",
      "        dest_region dest_size boarding_area   dept_time  wait_min  \\\n",
      "0              Asia       Hub  Gates 91-102  2018-12-31     115.0   \n",
      "1     Canada/Mexico     Small   Gates 50-59  2018-12-31     135.0   \n",
      "2           West US       Hub   Gates 40-48  2018-12-31      70.0   \n",
      "3           West US       Hub   Gates 20-39  2018-12-31     190.0   \n",
      "5           East US       Hub   Gates 50-59  2018-12-31     140.0   \n",
      "...             ...       ...           ...         ...       ...   \n",
      "2472        East US       Hub   Gates 50-59  2018-12-31     280.0   \n",
      "2473        West US       Hub   Gates 20-39  2018-12-31     165.0   \n",
      "2474        East US       Hub   Gates 70-90  2018-12-31      92.0   \n",
      "2475        West US     Small    Gates 1-12  2018-12-31      95.0   \n",
      "2476           Asia     Large    Gates 1-12  2018-12-31     220.0   \n",
      "\n",
      "         cleanliness         safety        satisfaction  \n",
      "0              Clean        Neutral      Very satisfied  \n",
      "1              Clean      Very safe      Very satisfied  \n",
      "2            Average  Somewhat safe             Neutral  \n",
      "3              Clean      Very safe  Somewhat satsified  \n",
      "5     Somewhat clean      Very safe      Very satisfied  \n",
      "...              ...            ...                 ...  \n",
      "2472  Somewhat clean        Neutral  Somewhat satsified  \n",
      "2473           Clean      Very safe      Very satisfied  \n",
      "2474           Clean      Very safe      Very satisfied  \n",
      "2475           Clean  Somewhat safe      Very satisfied  \n",
      "2476           Clean      Very safe  Somewhat satsified  \n",
      "\n",
      "[2474 rows x 13 columns]\n"
     ]
    }
   ],
   "source": [
    "# Find the cleanliness category in airlines not in categories\n",
    "cat_clean = set(airlines[\"cleanliness\"]).difference(categories[\"cleanliness\"])\n",
    "\n",
    "# Find rows with that category\n",
    "cat_clean_rows = airlines[\"cleanliness\"].isin(cat_clean)\n",
    "\n",
    "# Print rows with inconsistent category\n",
    "print(airlines[cat_clean_rows])\n",
    "\n",
    "# Print rows with consistent categories only\n",
    "print(airlines[~cat_clean_rows])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical variables\n",
    "\n",
    "In the last lesson we saw how categorical data has a value membership constraint, where columns need to have a predefined set of values.\n",
    "\n",
    "However, this is not the only set of problems we may encounter.\n",
    "\n",
    "When cleaning categorical data, some of the problems we may encounter include value inconsistency, the presence of too many categories that could be collapsed into one, and making sure data is of the right type. \n",
    "\n",
    "#### What type of errors could we have?\n",
    "\n",
    "1. Value inconsistency\n",
    "    * Inconsistent fields: `married`, `Maried`, `UNMARRIED`, `not married`, ...\n",
    "    * Trailing whitespace: `married `, ` married `, ...\n",
    "2. Collapsing too many categories\n",
    "    * Creating new groups: `0-20K`, `20-40K` categories from continuous household income data\n",
    "    * Mapping groups to new ones: mapping household income categories to 2 - `rich`, `poor`\n",
    "3. Making sure data is of type `category` (seen in Chapter 1)\n",
    "\n",
    "#### Value consistency\n",
    "\n",
    "A common problem is having categorical values that vary slightly in their capitalization.\n",
    "\n",
    "For a DataFrame, we can `groupby` the column and use the `.count()` method. \n",
    "\n",
    "``` python\n",
    "marriage_status.groupby('marriage_status').count()\n",
    "```\n",
    "\n",
    "``` text\n",
    "\t            household_income gender\n",
    "marriage_status\t\t\n",
    "MARRIED                      204    204\n",
    "UNMARRIED                    176    176\n",
    "married                      268    268\n",
    "unmarried                    352    352\n",
    "```\n",
    "\n",
    "To deal with this, normalize capitalization.\n",
    "\n",
    "``` python\n",
    "# capitalize\n",
    "marriage_status['marriage_status'] = marriage_status['marriage_status'].str.upper()\n",
    "marriage_status['marriage_status'].value_counts()\n",
    "```\n",
    "\n",
    "``` text\n",
    "UNMARRIED   528\n",
    "MARRIED     472\n",
    "```\n",
    "\n",
    "or\n",
    "\n",
    "``` python\n",
    "# lowercase\n",
    "marriage_status['marriage_status'] = marriage_status['marriage_status'].str.lower()\n",
    "marriage_status['marriage_status'].value_counts()\n",
    "```\n",
    "\n",
    "``` text\n",
    "unmarried   528\n",
    "married     472\n",
    "```\n",
    "\n",
    "Another common problem is leading or trailing whitespace.\n",
    "\n",
    "``` python\n",
    "demographics = demographics['marriage_status'].str.strip()\n",
    "demographics['marriage_status'].value_counts()\n",
    "```\n",
    "\n",
    "#### Collapsing data into categories\n",
    "\n",
    "Sometimes, we may want to create categories out of our data, such as creating household income groups from income data.\n",
    "\n",
    "For example, creating an income group column in the `demographics` DataFrame. This can be done in 2 ways.\n",
    "\n",
    "The first method utilizes the `qcut` function from pandas, which automatically divides our data based on its distribution into the number of categories we set in the `q` argument, we created the category names in the `group_names` list and fed it to the `labels` argument, returning the following.\n",
    "\n",
    "Notice that the first row actually misrepresents the actual income of the income group, as we didn't instruct `qcut` where our ranges actually lie.\n",
    "\n",
    "``` python\n",
    "# Using qcut\n",
    "import pandas as pd\n",
    "\n",
    "group_names = ['0-200K;', '200K-5OOK', '500K+']\n",
    "demographics['income_group'] = pd.qcut(demographics['householdincome'], q = 3, labels = group_names)\n",
    "\n",
    "# Print income_group column\n",
    "demographics[['income_group', 'household_income']]\n",
    "```\n",
    "\n",
    "``` text\n",
    "     category household_income\n",
    "0   20OK-500K           189243\n",
    "1       500K+           778533\n",
    "...\n",
    "```\n",
    "\n",
    "We can do this with the `cut` function instead, which lets us define category cutoff ranges with the `bins` argument. It takes in a list of cutoff points for each category, with the final one being infinity represented with `np.inf()`. From the output, we can see this is much more correct.\n",
    "\n",
    "``` python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# using cut() - create category ranges and names\n",
    "ranges = [0, 200_000, 500_000, np.inf]\n",
    "group_names = ['0-200K', '200K-500K', '500K+']\n",
    "\n",
    "# create income group column\n",
    "demographics['income_group'] = pd.cut(demographics['household_income'], bins = ranges, labels = group_names)\n",
    "dempgraphics[['income_group', 'household_income']]\n",
    "```\n",
    "\n",
    "``` text\n",
    "    category Income\n",
    "0     0-200K 189243\n",
    "1      500K+ 778533\n",
    "```\n",
    "\n",
    "#### Collapsing data into categories\n",
    "\n",
    "Assume we have a column containing the operating system of different devices, and contains these unique values.\n",
    "\n",
    "`operating_system` column is: `Microsoft`, `MacOS`, `IOS`, `Android`, `Linux`\n",
    "\n",
    "Say we want to collapse these categories into 2, DesktopOS, and MobileOS.\n",
    "\n",
    "`operating_system` column should become: `DesktopOS`, `MobileOS`\n",
    "\n",
    "We can do this using the `replace` method. It takes in a dictionary that maps each existing category to the category name you desire. \n",
    "\n",
    "``` python\n",
    "mapping = {\n",
    "    'Microsoft': 'DesktopOS',\n",
    "    'MacOS': 'DesktopOS',\n",
    "    'Linux': 'DesktopOS',\n",
    "    'IOS': 'MobileOS',\n",
    "    'Android': 'MobileOS'\n",
    "}\n",
    "\n",
    "devices['operating_system'] = devices['operating_system'].replace(mapping)\n",
    "devices['operating_system'].unique()\n",
    "```\n",
    "\n",
    "``` text\n",
    "array(['DesktopOS', 'MobileOS'], dtype=object)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inconsistent categories\n",
    "\n",
    "In this exercise, you'll be revisiting the `airlines` DataFrame from the previous lesson.\n",
    "\n",
    "As a reminder, the DataFrame contains flight metadata such as the airline, the destination, waiting times as well as answers to key questions regarding cleanliness, safety, and satisfaction on the San Francisco Airport.\n",
    "\n",
    "In this exercise, you will examine two categorical columns from this DataFrame, `dest_region` and `dest_size` respectively, assess how to address them and make sure that they are cleaned and ready for analysis. The pandas package has been imported as pd, and the `airlines` DataFrame is in your environment.\n",
    "\n",
    "##### Instructions 1/4\n",
    "\n",
    "* Print the unique values in `dest_region` and `dest_size` respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "airlines_csv = Path.cwd() / \"data/airlines.csv\"\n",
    "categories_csv = Path.cwd() / \"data/airlines_categories.csv\"\n",
    "\n",
    "airlines = pd.read_csv(airlines_csv)\n",
    "categories = pd.read_csv(categories_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Asia' 'Canada/Mexico' 'West US' 'East US' 'Midwest US' 'EAST US'\n",
      " 'Middle East' 'Europe' 'eur' 'Central/South America'\n",
      " 'Australia/New Zealand' 'middle east']\n",
      "['Hub' 'Small' '    Hub' 'Medium' 'Large' 'Hub     ' '    Small'\n",
      " 'Medium     ' '    Medium' 'Small     ' '    Large' 'Large     ']\n"
     ]
    }
   ],
   "source": [
    "# Print unique values of both columns\n",
    "print(airlines[\"dest_region\"].unique())\n",
    "print(airlines[\"dest_size\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Instructions 2/4\n",
    "\n",
    "**Question**\n",
    "\n",
    "From looking at the output, what do you think is the problem with these columns?\n",
    "\n",
    "**Possible answers**\n",
    "\n",
    "* The dest_region column has only inconsistent values due to capitalization.\n",
    "* The dest_region column has inconsistent values due to capitalization and has one value that needs to be remapped.\n",
    "* The dest_size column has only inconsistent values due to leading and trailing spaces.\n",
    "* Both 2 and 3 are correct.\n",
    "\n",
    "**Answer**\n",
    "Both 2 and 3 are correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Instructions 3/4\n",
    "\n",
    "* Change the capitalization of all values of `dest_region` to lowercase.\n",
    "* Replace the `'eur'` with `'europe'` in `dest_region` using the `.replace()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['asia' 'canada/mexico' 'west us' 'east us' 'midwest us' 'middle east'\n",
      " 'europe' 'central/south america' 'australia/new zealand']\n",
      "['Hub' 'Small' '    Hub' 'Medium' 'Large' 'Hub     ' '    Small'\n",
      " 'Medium     ' '    Medium' 'Small     ' '    Large' 'Large     ']\n",
      "['asia' 'canada/mexico' 'west us' 'east us' 'midwest us' 'middle east'\n",
      " 'europe' 'central/south america' 'australia/new zealand']\n",
      "['Hub' 'Small' '    Hub' 'Medium' 'Large' 'Hub     ' '    Small'\n",
      " 'Medium     ' '    Medium' 'Small     ' '    Large' 'Large     ']\n"
     ]
    }
   ],
   "source": [
    "# Lower dest_region column and then replace \"eur\" with \"europe\"\n",
    "airlines[\"dest_region\"] = airlines[\"dest_region\"].str.lower()\n",
    "airlines[\"dest_region\"] = airlines[\"dest_region\"].replace({\"eur\": \"europe\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Instructions 4/4\n",
    "\n",
    "* Strip white spaces from the `dest_size` column using the `.strip()` method.\n",
    "* Verify that the changes have been into effect by printing the unique values of the columns using `.unique()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lower dest_region column and then replace \"eur\" with \"europe\"\n",
    "airlines[\"dest_region\"] = airlines[\"dest_region\"].str.lower()\n",
    "airlines[\"dest_region\"] = airlines[\"dest_region\"].replace({\"eur\": \"europe\"})\n",
    "\n",
    "# Remove white spaces from `dest_size`\n",
    "airlines[\"dest_size\"] = airlines[\"dest_size\"].str.strip()\n",
    "\n",
    "# Verify changes have been effected\n",
    "print(airlines[\"dest_region\"].unique())\n",
    "print(airlines[\"dest_size\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remapping categories\n",
    "\n",
    "To better understand survey respondents from `airlines`, you want to find out if there is a relationship between certain responses and the day of the week and wait time at the gate.\n",
    "\n",
    "The `airlines` DataFrame contains the `day` and `wait_min` columns, which are categorical and numerical respectively. The `day` column contains the exact day a flight took place, and `wait_min` contains the amount of minutes it took travelers to wait at the gate. To make your analysis easier, you want to create two new categorical variables:\n",
    "\n",
    "* `wait_type`: `'short'` for 0-60 min, `'medium'` for 60-180 and `long` for 180+\n",
    "* `day_week`: `'weekday'` if `day` is in the weekday, `'weekend'` if day is in the weekend.\n",
    "\n",
    "The `pandas` and `numpy` packages have been imported as pd and np. Let's create some new categorical data!\n",
    "\n",
    "##### Instructions\n",
    "\n",
    "* Create the ranges and labels for the `wait_type` column mentioned in the description.\n",
    "* Create the `wait_type` column by from `wait_min` by using `pd.cut()`, while inputting `label_ranges` and `label_names` in the correct arguments.\n",
    "* Create the `mapping` dictionary mapping weekdays to `'weekday'` and weekend days to `'weekend'`.\n",
    "* Create the `day_week` column by using `.replace()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ranges for categories\n",
    "label_ranges = [0, 60, 180, np.inf]\n",
    "label_names = [\"short\", \"medium\", \"long\"]\n",
    "\n",
    "# Create wait_type column\n",
    "airlines[\"wait_type\"] = pd.cut(\n",
    "    airlines[\"wait_min\"], bins=label_ranges, labels=label_names\n",
    ")\n",
    "\n",
    "# Create mappings and replace\n",
    "mappings = {\n",
    "    \"Monday\": \"weekday\",\n",
    "    \"Tuesday\": \"weekday\",\n",
    "    \"Wednesday\": \"weekday\",\n",
    "    \"Thursday\": \"weekday\",\n",
    "    \"Friday\": \"weekday\",\n",
    "    \"Saturday\": \"weekend\",\n",
    "    \"Sunday\": \"weekend\",\n",
    "}\n",
    "\n",
    "airlines[\"day_week\"] = airlines[\"day\"].replace(mappings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning text data\n",
    "\n",
    "Text data is *very* common.\n",
    "\n",
    "``` python\n",
    "phones = pd.read_csv('phones.csv')\n",
    "print(phones)\n",
    "```\n",
    "\n",
    "```text\n",
    "             Full name     Phone number\n",
    "0      Noelani A. Gray 001-702-397-5143\n",
    "1       Myles Z. Gomez 001-329-485-0540\n",
    "2         Gil B. Silva 001-195-492-2338\n",
    "3   Prescott D. Hardin  +1-297-996-4904   <-- inconsistent data format\n",
    "4   Benedict G. Valdez 001-969-820-3536\n",
    "5     Reece M. Andrews             4138   <-- length violation\n",
    "6       Hayfa E. Keith 001-536-175-8444\n",
    "7      Hedley I. Logan 001-681-552-1823\n",
    "8     Jack W. Carrillo 001-910-323-5265\n",
    "9      Lionel M. Davis 001-143-119-9210\n",
    "```\n",
    "\n",
    "* there are phone number values that begin with `00` or `+`\n",
    "* there is one entry where the phone number is 4 digits, which is non-existent\n",
    "* there are dashes across the phone number column\n",
    "\n",
    "If we wanted to feed these phone numbers into an automated call system, or create a report discussing the distribution of users by area code, we couldn't really do so without uniform phone numbers. \n",
    "\n",
    "Ideally, we want the following:\n",
    "\n",
    "```text\n",
    "             Full name     Phone number\n",
    "0      Noelani A. Gray    0017023975143\n",
    "1       Myles Z. Gomez    0013294850540\n",
    "2         Gil B. Silva    0011954922338\n",
    "3   Prescott D. Hardin    0012979964904\n",
    "4   Benedict G. Valdez    0019698203536\n",
    "5     Reece M. Andrews              NaN\n",
    "6       Hayfa E. Keith    0015361758444\n",
    "7      Hedley I. Logan    0016815521823\n",
    "8     Jack W. Carrillo    0019103235265\n",
    "9      Lionel M. Davis    0011431199210\n",
    "```\n",
    "\n",
    "```python\n",
    "# replace \"+\" with \"00\"\n",
    "phones['Phone number'] = phones['Phone number'].str.replace('+', '00')\n",
    "\n",
    "# replace \"-\" with nothing\n",
    "phones['Phone number'] = phones['Phone number'].str.replace('-', '')\n",
    "\n",
    "# replace phone numbers with less than 10 digits with NaN\n",
    "digits = phones['Phone number'].str.len\n",
    "phones.loc[digits < 10, \"Phone number\"] = np.nan\n",
    "\n",
    "# sanity check\n",
    "sanity_check = phones['Phone number'].str.len()\n",
    "assert sanity_check.min() >= 10\n",
    "assert phones['Phone number'].str.contains(\"+|-\").any() == False\n",
    "```\n",
    "\n",
    "#### But what about more complicated examples?\n",
    "\n",
    "```python\n",
    "phones.head()\n",
    "```\n",
    "\n",
    "```text\n",
    "           Full name    Phone number\n",
    "0      Olga Robinson  +(01706)-25891\n",
    "1        Justina Kim    +0500-571437\n",
    "2     Tamekah Henson      +0800-1111\n",
    "3      Miranda Solis   +07058-879063\n",
    "4   Caldwell Gilliam  +(016977)-8424\n",
    "```\n",
    "\n",
    "RegEX!!!\n",
    "\n",
    "```python\n",
    "phones['Phone number'] = phones['Phone number'].str.replace(r\\'D+', '')\n",
    "phones.head()\n",
    "```\n",
    "\n",
    "```text\n",
    "           Full name    Phone number\n",
    "0      Olga Robinson      0170625891\n",
    "1        Justina Kim      0500571437\n",
    "2     Tamekah Henson       +08001111\n",
    "3      Miranda Solis     07058879063\n",
    "4   Caldwell Gilliam      0169778424\n",
    "``````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing titles and taking names\n",
    "\n",
    "While collecting survey respondent metadata in the `airlines` DataFrame, the full name of respondents was saved in the `full_name` column. However upon closer inspection, you found that a lot of the different names are prefixed by honorifics such as `\"Dr.\"`, `\"Mr.\"`, `\"Ms.\"` and `\"Miss\"`.\n",
    "\n",
    "Your ultimate objective is to create two new columns named `first_name` and `last_name`, containing the first and last names of respondents respectively. Before doing so however, you need to remove honorifics.\n",
    "\n",
    "The `airlines` DataFrame is in your environment, alongside pandas as `pd`.\n",
    "\n",
    "##### Instructions\n",
    "\n",
    "* Remove `\"Dr.\"`, `\"Mr.\"`, `\"Miss\"` and `\"Ms.\"` from `full_name` by replacing them with an empty string `\"\"` in that order.\n",
    "* Run the `assert` statement using `.str.contains()` that tests whether `full_name` still contains any of the honorifics.\n",
    "\n",
    "**Note:** our version of `airlines` did not have the `full_name` column, added with the help of [Mockaroo](https://www.mockaroo.com/27996250).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "airlines_csv = Path.cwd() / \"data/airlines.csv\"\n",
    "categories_csv = Path.cwd() / \"data/airlines_categories.csv\"\n",
    "\n",
    "airlines = pd.read_csv(airlines_csv)\n",
    "categories = pd.read_csv(categories_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace \"Dr.\" with empty string \"\"\n",
    "airlines[\"full_name\"] = airlines[\"full_name\"].str.replace(\"Dr.\", \"\").str.strip()\n",
    "\n",
    "# Replace \"Mr.\" with empty string \"\"\n",
    "airlines[\"full_name\"] = airlines[\"full_name\"].str.replace(\"Mr.\", \"\").str.strip()\n",
    "\n",
    "# Replace \"Miss\" with empty string \"\"\n",
    "airlines[\"full_name\"] = airlines[\"full_name\"].str.replace(\"Miss\", \"\").str.strip()\n",
    "\n",
    "# Replace \"Ms.\" with empty string \"\"\n",
    "airlines[\"full_name\"] = airlines[\"full_name\"].str.replace(\"Ms.\", \"\").str.strip()\n",
    "\n",
    "airlines.loc[airlines[\"full_name\"].str.contains(r\"Ms\\.|Mr\\.|Miss|Dr\\.\"), \"full_name\"]\n",
    "\n",
    "# Assert that full_name has no honorifics\n",
    "assert not airlines[\"full_name\"].str.contains(r\"Ms\\.|Mr\\.|Miss|Dr\\.\").any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keeping it descriptive\n",
    "\n",
    "To further understand travelers' experiences in the San Francisco Airport, the quality assurance department sent out a qualitative questionnaire to all travelers who gave the airport the worst score on all possible categories. The objective behind this questionnaire is to identify common patterns in what travelers are saying about the airport.\n",
    "\n",
    "Their response is stored in the `survey_response` column. Upon a closer look, you realized a few of the answers gave the shortest possible character amount without much substance. In this exercise, you will isolate the responses with a character count higher than **40**, and make sure your new DataFrame contains responses with **40** characters or more using an `assert` statement.\n",
    "\n",
    "The `airlines` DataFrame is in your environment, and `pandas` is imported as `pd`.\n",
    "\n",
    "##### Instructions\n",
    "\n",
    "* Using the `airlines` DataFrame, store the length of each instance in the `survey_response` column in `resp_length` by using `.str.len()`.\n",
    "* Isolate the rows of `airlines` with `resp_length` higher than `40`.\n",
    "* Assert that the smallest `survey_response` length in `airlines_survey` is now bigger than `40`.\n",
    "\n",
    "**Note:** had to generate `survey_response` using [Mockaroo](https://www.mockaroo.com/c0c8f670)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3       Pellentesque eget nunc. Donec quis orci eget o...\n",
      "5       Nullam orci pede venenatis non sodales sed tin...\n",
      "7       Morbi non lectus. Aliquam sit amet diam in mag...\n",
      "9           Suspendisse potenti. In eleifend quam a odio.\n",
      "10      Integer aliquet massa id lobortis convallis to...\n",
      "                              ...                        \n",
      "2470    Nulla facilisi. Cras non velit nec nisi vulput...\n",
      "2471    In hac habitasse platea dictumst. Maecenas ut ...\n",
      "2472     Quisque ut erat. Curabitur gravida nisi at nibh.\n",
      "2473    Ut tellus. Nulla ut erat id mauris vulputate e...\n",
      "2476    Proin leo odio porttitor id consequat in conse...\n",
      "Name: survey_response, Length: 1509, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Store length of each row in survey_response column\n",
    "resp_length = airlines[\"survey_response\"].str.len()\n",
    "\n",
    "# Find rows in airlines where resp_length > 40\n",
    "airlines_survey = airlines[resp_length > 40]\n",
    "\n",
    "# Assert minimum survey_response length is > 40\n",
    "assert airlines_survey[\"survey_response\"].str.len().min() > 40\n",
    "\n",
    "# Print new survey_response column\n",
    "print(airlines_survey[\"survey_response\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced data problems\n",
    "\n",
    "In this chapter, you’ll dive into more advanced data cleaning problems, such as ensuring that weights are all written in kilograms instead of pounds. You’ll also gain invaluable skills that will help you verify that values have been added correctly and that missing values don’t negatively impact your analyses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uniformity\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
